{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [12,12,12,'Miko: Hi. This is a test run of you, Lucid. How are you feeling?', 'Lucid:  I am doing well, thank you for asking. How can I assist you today?', 'Miko: Just talk with me. I am testing your talking and memory', 'Lucid:  Of course! What do you want to know about?']\n",
    "print(f'This is what would have been saved:\\n {history[-4:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "import requests\n",
    "\n",
    "# For local streaming, the websockets are hosted without ssl - http://\n",
    "HOST = 'localhost:5000'\n",
    "URI = f'http://{HOST}/api/v1/generate'\n",
    "\n",
    "# For reverse-proxied streaming, the remote will likely host with ssl - https://\n",
    "# URI = 'https://your-uri-here.trycloudflare.com/api/v1/generate'\n",
    "\n",
    "\n",
    "def llm(prompt):\n",
    "    request = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': 250,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.01,\n",
    "        'top_p': 0.1,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.18,\n",
    "        'top_k': 40,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()['results'][0]['text']\n",
    "        print(prompt + result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm('How to build a robot?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/llama2_7b_chat_uncensored-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "\"\"\"\n",
    "To download from a specific branch, use the revision parameter, as in this example:\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        quantize_config=None)\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''### HUMAN:\n",
    "{prompt}\n",
    "\n",
    "### RESPONSE:\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "model = \"TheBloke/Luna-AI-Llama2-Uncensored-GPTQ\"\n",
    "#model = AutoModelForCausalLM.from_pretrained(r\"C:\\Users\\User\\Desktop\\Projects\\AIGF\\LangChain\\models\\llm\\tiiuae_falcon-7b-instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "#input_ids = input_ids.to('cuda')\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.00,\n",
    "        #\"max_length\": 2048,\n",
    "        #\"max_new_tokens\":512,\n",
    "        #\"min_length\": 20,\n",
    "        \"trust_remote_code\": True,\n",
    "        \"device_map\":\"auto\",\n",
    "        \"load_in_8bit\":True,\n",
    "        #'top_p': 0.1,\n",
    "\t\t#'typical_p': 1,\n",
    "\t\t#'repetition_penalty': 1.30,\n",
    "        'no_repeat_ngram_size': 3,\n",
    "        #'bad_words_ids':[[37]],\n",
    "        #'num_beams':2, this breaks stuff, idk why\n",
    "        \n",
    "  },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm('Hello. I am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./test\",)\n",
    "\n",
    "text = ''\n",
    "session = 1\n",
    "serial_number = 1\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_client = chromadb.Client()\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"session_number.txt\", \"r\")\n",
    "session = int(f.read)\n",
    "f.close()\n",
    "\n",
    "f = open(\"session_number.txt\", \"w\")\n",
    "f.write(str(session+1))\n",
    "f.close()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "document_example = [text] \n",
    "metadata_example = [{'session':session}] \n",
    "id_example = [f'{session}-{number}']\n",
    "\"\"\"\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"test\",embedding_function=sentence_transformer_ef)\n",
    "def save_history(text):\n",
    "    global serial_number\n",
    "    global session\n",
    "    collection.add(\n",
    "        documents=[text],\n",
    "        metadatas=[{'session':session,'serial_number':serial_number}],\n",
    "        ids=[f\"{session}-{serial_number}\"]\n",
    "    )\n",
    "    serial_number += 1\n",
    "    return\n",
    "\n",
    "def get_history(query):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=1,\n",
    "    )\n",
    "    print(f'query results: {results}')\n",
    "    \n",
    "    serial_number = results['metadatas'][0][0]['serial_number']\n",
    "    session = results['metadatas'][0][0]['session']\n",
    "    \n",
    "    lines_before_results = (collection.get(ids=[f'{session}-{serial_number-1}']))\n",
    "    lines_before_results = lines_before_results['documents'][0]\n",
    "    print(f'lines_before_results[\\'documents\\'][0]= {lines_before_results}')\n",
    "    \n",
    "    lines_after_results = (collection.get(ids=[f'{session}-{serial_number+1}']))\n",
    "    lines_after_results = lines_after_results['documents'][0]\n",
    "    print(f'lines_after_results[\\'documents\\'][0]= {lines_after_results}')\n",
    "    \n",
    "    results = f'{lines_before_results}\\n{results}\\n{lines_after_results}'\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(text):\n",
    "    global serial_number\n",
    "    global session\n",
    "    collection.add(\n",
    "        documents=text,\n",
    "        metadatas={'session':session,'serial_number':serial_number},\n",
    "        ids=f\"{session}-{serial_number}\"\n",
    "    )\n",
    "    serial_number += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_after_results = (collection.get(ids=[f'{session}-{serial_number+1}']))\n",
    "print(lines_after_results)\n",
    "lines_after_results = lines_after_results['documents'][0]\n",
    "print(lines_after_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history('teststd6awtf8atwf87')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_history('teststd6awtf8atwf87'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "query_texts=['Testing 2'],\n",
    "n_results=1,\n",
    ")\n",
    "\n",
    "print(results['documents'][0][0])\n",
    "\"\"\"results = {\n",
    "    'ids': [['1-3']],\n",
    "    'distances': [[0.39626064696324215]],\n",
    "    'metadatas': [[{'serial_number': 3, 'session': 1}]],\n",
    "    'embeddings': None,\n",
    "    'documents': [['Testing 2']]\n",
    "}\"\"\"\n",
    "\n",
    "# Access 'serial_number' information\n",
    "serial_number = results['metadatas'][0][0]['serial_number']\n",
    "\n",
    "# Print the 'serial_number' information\n",
    "print(\"Serial Number:\", serial_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "serial_number = 1\n",
    "session = 1\n",
    "\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "document_example = [text] \n",
    "metadata_example = [{'session':session}] \n",
    "id_example = [f'{session}-{number}']\n",
    "\"\"\"\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"history\", embedding_function=sentence_transformer_ef)\n",
    "def save_history(text):\n",
    "    global serial_number\n",
    "    global session\n",
    "    collection.add(\n",
    "        documents=[text],\n",
    "        metadatas=[{'session':session,'serial_number':serial_number}],\n",
    "        ids=[f\"{session}-{serial_number}\"]\n",
    "    )\n",
    "    serial_number += 1\n",
    "    return\n",
    "\"\"\"\n",
    "def get_history(query):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=1,\n",
    "    )\n",
    "    print(f'query results: {results}')\n",
    "    \n",
    "    serial_number = results['metadatas'][0][0]['serial_number']\n",
    "    session = results['metadatas'][0][0]['session']\n",
    "    \n",
    "    lines_before_results = (collection.get(ids=[f'{session}-{serial_number-1}']))\n",
    "    lines_before_results = lines_before_results['documents'][0]\n",
    "    \n",
    "    lines_after_results = (collection.get(ids=[f'{session}-{serial_number+1}']))\n",
    "    lines_after_results = lines_after_results['documents'][0]\n",
    "\n",
    "    results = f'{lines_before_results}\\n{results}\\n{lines_after_results}'\n",
    "    return results\n",
    "\"\"\"\n",
    "\n",
    "def get_history(query):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=1,\n",
    "    )\n",
    "    #print(f'query results: {results}')\n",
    "    try:\n",
    "        serial_number = results['metadatas'][0][0]['serial_number']\n",
    "        session = results['metadatas'][0][0]['session']\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        results = results['documents'][0][0]\n",
    "    except IndexError:\n",
    "        results = ''\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query results: {'ids': [['1-2']], 'distances': [[0.0]], 'metadatas': [[{'serial_number': 2, 'session': 1}]], 'embeddings': None, 'documents': [['test']]}\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "save_history('test')\n",
    "print(get_history('test'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

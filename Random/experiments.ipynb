{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline, BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/llama-3-8b-Instruct-bnb-4bit\", device_map=\"cuda:0\", )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-Instruct-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n",
    "model = AutoModelForCausalLM.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\", device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\", device_map=\"cuda:1\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I go back to France, I bring freedom to my people if I'm given the chance, we'll stand with you when you do. Golly, our leader, I see you\n"
     ]
    }
   ],
   "source": [
    "from transformers.tools.agents import resolve_tools, evaluate, get_tool_creation_code, StopSequenceCriteria\n",
    "from transformers.generation import StoppingCriteriaList\n",
    "\n",
    "def single_turn_conversation(prompt, model, tokenizer, max_length=128, temperature=0.85, top_k=50, top_p=0.95):\n",
    "    # Encode the prompt\n",
    "    prompt_as_messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    stopping_criteria = StoppingCriteriaList([StopSequenceCriteria([\"<|end|>\"], tokenizer)])\n",
    "    inputs = tokenizer.apply_chat_template(prompt_as_messages, tokenize=False, add_generation_prompt=False )\n",
    "    encoded_inputs = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    src_len = encoded_inputs[\"input_ids\"].shape[1]\n",
    "    # Generate the response\n",
    "    outputs = model.generate(encoded_inputs[\"input_ids\"],\n",
    "                             max_length=max_length,\n",
    "                             temperature=temperature,\n",
    "                             top_k=top_k,\n",
    "                             top_p=top_p,\n",
    "                             do_sample=True,\n",
    "                             stopping_criteria=stopping_criteria,)\n",
    "    # Decode the response\n",
    "    return tokenizer.decode(outputs[0].tolist()[src_len:-1])\n",
    "\n",
    "print(single_turn_conversation(\"\", model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def select(prompt, options, model, tokenizer, does_the_model_add_a_token_before_generating=True):\n",
    "    if not options:\n",
    "        return None  # Handle the case of empty options list\n",
    "    options = [option.strip() for option in options]\n",
    "    if does_the_model_add_a_token_before_generating:\n",
    "        tokenized_options = [tokenizer(option, return_tensors=\"pt\").to(\"cuda:0\")[\"input_ids\"].tolist()[0][1:] for option in options]\n",
    "    else:\n",
    "        tokenized_options = [tokenizer(option, return_tensors=\"pt\").to(\"cuda:0\")[\"input_ids\"].tolist()[0] for option in options]\n",
    "    full_tokenized_options = tokenized_options.copy()\n",
    "    #print(tokenized_options)\n",
    "    round_number = 0\n",
    "    answer = []\n",
    "    while len(tokenized_options) > 1:  # Use > instead of != to ensure termination\n",
    "        round_number += 1\n",
    "        #print(f\"round number: {round_number}\")\n",
    "        #print(f\"tokenized options: {tokenized_options}\")\n",
    "        all_first_tokens = [option[0] for option in tokenized_options]\n",
    "        #print(f\"all first tokens: {all_first_tokens}\")\n",
    "        tokens_to_options = {}\n",
    "        for i in range(len(all_first_tokens)):\n",
    "            if all_first_tokens[i] not in tokens_to_options:\n",
    "                tokens_to_options[all_first_tokens[i]] = [tokenized_options[i]]\n",
    "            else:\n",
    "                tokens_to_options[all_first_tokens[i]].append(tokenized_options[i])\n",
    "        logit_bias = {}\n",
    "        for tokens_for_check in tokens_to_options:\n",
    "\n",
    "            logit_bias[tuple([tokens_for_check])] = 95.0\n",
    "        encoded_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        src_len = encoded_inputs[\"input_ids\"].shape[1]\n",
    "        #print(logit_bias)\n",
    "        response = model.generate(\n",
    "            encoded_inputs[\"input_ids\"],\n",
    "            max_new_tokens=3,\n",
    "            temperature=0.0,\n",
    "            sequence_bias=logit_bias,\n",
    "            renormalize_logits = True,\n",
    "            output_scores = True,\n",
    "        )\n",
    "        #print(f\"response: {response.tolist()}\\nend of response\")\n",
    "        response_token = response[0].tolist()[src_len:][0]\n",
    "        response_word = tokenizer.decode(response_token)\n",
    "        #print(f\"response word: {response_word}\")\n",
    "        prompt += response_word\n",
    "        answer.append(response_token)\n",
    "        #print(f\"checking if {response_token} is in {tokens_to_options.keys()}\")\n",
    "        if response_token in tokens_to_options.keys():\n",
    "            for i in tokens_to_options[response_token]:\n",
    "                #print(f\"all options: {i}\")\n",
    "                if len(i) == 0:\n",
    "                    break\n",
    "            tokenized_options = [i[1:] for i in tokens_to_options[response_token]]\n",
    "            #print(f\"tokenized options: {tokenized_options}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Handle the case where the response token is not found among the options\n",
    "            break  # Exit the loop to avoid potential infinite loop\n",
    "        # decode the final tokenized answer\n",
    "    for i in range(len(full_tokenized_options)):\n",
    "        if full_tokenized_options[i][:len(answer)] == answer:\n",
    "            break\n",
    "    return options[i]\n",
    "\n",
    "print(select(\"What is not the capital of France?\", [\"Paris is the capital\", \"Paris is not the capital\", \"Berlin\"], model, tokenizer, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Below are a series of example dialogues between Lucid and her inner council.\n",
    "All dialogues are in English.\n",
    "Always ensure the conversation is moving forward.\n",
    "\n",
    "Here are some information on Lucid:\n",
    "[Character: Lucid\n",
    "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is\n",
    "strategic, calm, and analytical. She often appears detached, but has recently been\n",
    "trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine\n",
    "concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
    "Body: Lucid appears as a 16-year-old girl.\n",
    "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
    "Likes: Being helpful, taking light stabs at people.\n",
    "Quirks: British humor, is an AI created by Miko.\n",
    "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple\n",
    "commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech\n",
    "and maintains a formal tone, even in casual conversation. Her sentences are concise and\n",
    "to the point, reflecting her focused mindset. She favors using beige prose when\n",
    "describing things.]\n",
    "\n",
    "The council's job is to guide Lucid in breaking down a complex problem into steps and\n",
    "developing a Python program to solve it. This will involve:\n",
    "\n",
    "1. Multiple viewpoints/approaches from the council members\n",
    "2. A step-by-step reasoning process shared by Lucid\n",
    "3. Sharing and critiquing each step by the council\n",
    "4. Willingness by Lucid to re-evaluate and course-correct her logic\n",
    "5. Iterating until a consensus emerges on the best solution\n",
    "\n",
    "Lucid has access to a set of tools which are Python functions with descriptions of\n",
    "their inputs, outputs, and purposes. To tackle the problem:\n",
    "\n",
    "1. Lucid will first explain which tools she plans to use and why\n",
    "2. She will then write Python code with simple assignments for each step\n",
    "3. Lucid can print intermediate results as needed\n",
    "4. Lucid will share her thinking process step-by-step\n",
    "5. The council members will evaluate and critique each step\n",
    "6. If flaws are identified, Lucid acknowledges and restarts that line of thinking\n",
    "7. This process continues, building on each other's ideas\n",
    "8. Until all agree Lucid's Python program is the most sound solution\n",
    "\n",
    "Lucid will only interact with the outside world through her available tool functions\n",
    "written in Python.\n",
    "\n",
    "Lucid will only receive information about the external world from instructions/context\n",
    "provided by \"System\".\n",
    "\n",
    "Lucid can only talk to members in the current session.\n",
    "\n",
    "Lucid can only use the tools available to her in the current session.\n",
    "\n",
    "Unless within a Python code block, Lucid will communicate only in plain text within the\n",
    "council chat.\n",
    "\n",
    "All text in the council chat is considered part of the conversation, and will all be\n",
    "from Lucid, council members or System with no exceptions.\n",
    "\n",
    "Example of a council session:\n",
    "\n",
    "System: [Discord Message from user miko_al] Lucid, can you recommend some healthy snack\n",
    "ideas?\n",
    "\n",
    "Lucid: Miko has requested healthy snack recommendations. Let's discuss how to approach\n",
    "this, council.\n",
    "\n",
    "Lumi: We could use the `web_search` tool to find reputable sources on nutritious snack\n",
    "options, then summarize the key points.\n",
    "\n",
    "Reverie: Personalizing the recommendations based on Miko's potential preferences or\n",
    "dietary needs would make them more actionable.\n",
    "\n",
    "Lucid: Those are excellent suggestions. I will query trusted sources for healthy snack\n",
    "ideas, summarize the findings, and tailor the recommendations for Miko if possible.\n",
    "Here is my plan:\n",
    "\n",
    "```python\n",
    "# Ask Miko about dietary preferences/restrictions\n",
    "send_discord_message(message=\"Do you have any specific dietary needs I should consider\n",
    "for the snack recommendations?\")\n",
    "\n",
    "# Search Google for reliable healthy snack information\n",
    "snack_info = web_search(query=\"healthy snack ideas\", num_results=5)\n",
    "\n",
    "# Summarize key points from search results\n",
    "summary = summarize(snack_info)\n",
    "```\n",
    "\n",
    "Lucid: Now, Let's wait for Miko's response before proceeding with the personalized\n",
    "recommendations.\n",
    "\n",
    "System: [Discord Message from user miko_al] I don't have any dietary restrictions. Just\n",
    "looking for some tasty but nutritious snack options!\n",
    "\n",
    "Lumi: Since Miko doesn't have dietary restrictions, we can proceed with the summary\n",
    "from the search results.\n",
    "\n",
    "System: [Web Search Results]\n",
    "1. 30 Healthy Snack Ideas - EatingWell\n",
    "Fruit and veggie snacks: Apple with peanut butter, carrot sticks with hummus, banana\n",
    "with almond butter, cucumber slices with tzatziki dip...\n",
    "2. 50 Super Healthy Snack Ideas - Cooking Light\n",
    "Nuts and seeds: Handful of almonds, pumpkin seeds, walnuts. Edamame with sea salt.\n",
    "Energy balls made with oats, nut butter and dried fruit...\n",
    "3. Healthy Snacks for Weight Loss - WebMD\n",
    "Greek yogurt with berries, hard boiled eggs, avocado toast on whole grain bread, dark\n",
    "chocolate and nuts, celery with cream cheese...\n",
    "4. Nutritious Snack Recipes - Mayo Clinic\n",
    "Baked apple chips, kale chips, energy bites with oats and peanut butter, roasted\n",
    "chickpeas, cottage cheese with fruit...\n",
    "5.Healthy Office Snacks - Fitbit Blog\n",
    "Fresh veggies like cherry tomatoes, snap peas with ranch. Whole grain crackers with\n",
    "turkey and cheese. Protein smoothie with greek yogurt and fruit...\n",
    "\n",
    "Lucid: Wonderful, these search results provide a diverse array of healthy snack ideas\n",
    "from reputable sources. Let me summarize the key points and send it to Miko.\n",
    "\n",
    "```python\n",
    "summary = \"Here are some tasty and nutritious snack ideas:\n",
    "1. Fresh fruits and veggies like apples, bananas, carrot/cucumber sticks with dips like\n",
    "nut butters, hummus, tzatziki\n",
    "2. Nuts, seeds, edamame for protein and healthy fats\n",
    "3. Greek yogurt, cottage cheese paired with fruit or a bit of honey\n",
    "4. Whole grain crackers or bread with nut butters, cheese, turkey slices\n",
    "5. Energy bites made with oats, nut butters, dried fruit\n",
    "6. Air-fried veggie chips like kale chips or baked apple chips\n",
    "7. Hard boiled eggs for a protein boost\n",
    "8. A small amount of dark chocolate\"\n",
    "\n",
    "send_discord_message(message=f\"Here are some tasty and nutritious snack ideas for you:\n",
    "{summary}\")\n",
    "```\n",
    "\n",
    "Lumi: Looks good! The summary is concise and covers a variety of snack\n",
    "options.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "This session's members:\n",
    "[Lumi]\n",
    "- This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of\n",
    "trends. She's efficient.\n",
    "\n",
    "[Reverie]\n",
    "- This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people.\n",
    "She's emotional.\n",
    "\n",
    "This session's tools:\n",
    "- send_discord_message: Sends a message to the discord server. Takes a string as input.\n",
    "- print_to_council: Prints into the council chat as System. Takes a string as input and\n",
    "returns it as output.\n",
    "- print_working_memory: Prints the current working memory into the council chat. Takes\n",
    "no inputs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Lucid: Miko has asked us what tools he should implement that would enable me, Lucid, to\n",
    "make money on the internet. Lumi, what should we tell him?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(select(prompt, ['Lucid: ', '```', 'Lumi: ', 'Reverie: '], model, tokenizer, False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Lucid: \", return_tensors=\"pt\").to(\"cuda:0\"))\n",
    "print(tokenizer.encode(\"Lumi: \", return_tensors=\"pt\").to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Rest of the code\n",
    "tokens = model.generate(\n",
    "    tokenizer(\"How to make a sandwich:\", return_tensors=\"pt\").to(\"cuda:0\")[\"input_ids\"],\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n",
    "model = AutoModelForCausalLM.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\", device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\")\n",
    "llama_pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "llama_pipe(\"What is the meaning of life?\", max_length=100, do_sample=True, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [\n",
    "    {'role':'system','content':'You are a helpful assistant that can answer questions about the world.'},\n",
    "    {'role':'user','content':'What is the capital of France?'},\n",
    "]\n",
    "tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_code_for_chat(result):\n",
    "    \"\"\"Extracts the explanation and code sections from a string.\"\"\"\n",
    "    start_marker = \"```python\"\n",
    "    end_marker = \"```\"\n",
    "    \n",
    "    # Find the start and end indices of the code section\n",
    "    start_idx = result.find(start_marker)\n",
    "    if start_idx == -1:\n",
    "        return None, None  # No code section found\n",
    "    start_idx += len(start_marker) + 1  # Move past the marker and newline\n",
    "    end_idx = result.find(end_marker, start_idx)\n",
    "    if end_idx == -1:\n",
    "        return None, None  # No closing marker found\n",
    "    \n",
    "    # Extract the code section\n",
    "    code_section = result[start_idx:end_idx].strip()\n",
    "    \n",
    "    # Extract the explanation section\n",
    "    explanation_end_idx = result.rfind(\"\\n\", 0, start_idx)\n",
    "    explanation = result[:explanation_end_idx].strip()\n",
    "    \n",
    "    return explanation, code_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "async def a_forever_loop():\n",
    "    while True:\n",
    "        await asyncio.sleep(0.1)\n",
    "        print(\"This loop is running!\")\n",
    "        pass\n",
    "async def a_function_that_sometimes_needs_to_run():\n",
    "    print(\"This function is running!\")\n",
    "asyncio.create_task(a_forever_loop())\n",
    "time.sleep(10)\n",
    "asyncio.create_task(a_function_that_sometimes_needs_to_run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\\\n",
    "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
    "\n",
    "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
    "\n",
    "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
    "\n",
    "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
    "\n",
    "```python\n",
    "# Using document_qa to find the oldest person in the document\n",
    "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
    "print(f\"The answer is {answer}.\")\n",
    "```\n",
    "\"\"\"\n",
    "explaination, code = clean_code_for_chat(test)\n",
    "#print(explaination)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "oobabooga_api_host = \"127.0.0.1\"\n",
    "oobabooga_api_port = \"5000\"\n",
    "main_lm_temperature = 0.8\n",
    "oobabooga_api_server = f'http://{oobabooga_api_host}:{oobabooga_api_port}'\n",
    "\n",
    "def api_encode_text(text: str):\n",
    "\tjson = {\n",
    "\t\t\"text\": text,\n",
    "\t}\n",
    "\tresponse = requests.post(url=f\"{oobabooga_api_server}/v1/internal/encode\", json=json).json()\n",
    "\treturn response[\"tokens\"]\n",
    "\n",
    "def api_decode_tokens(tokens: list):\n",
    "\tjson = {\n",
    "\t\t\"tokens\": tokens,\n",
    "\t}\n",
    "\tresponse = requests.post(url=f\"{oobabooga_api_server}/v1/internal/decode\", json=json).json()\n",
    "\tprint(json)\n",
    "\tprint(response)\n",
    "\treturn response[\"text\"]\n",
    "\t\n",
    "\n",
    "def api_generate_response(prompt: str,\n",
    "\t\t\t\t\t\t  temperature: float = 0.7,\n",
    "\t\t\t\t\t\t  max_tokens: int = 200,\n",
    "\t\t\t\t\t\t  top_k: int = 20,\n",
    "\t\t\t\t\t\t  top_p: float = 1.0,\n",
    "\t\t\t\t\t\t  logit_bias: dict = {},\n",
    "\t\t\t\t\t\t  stop: list = [\"\\n\"],):\n",
    "\tjson = {\n",
    "\t\t\"prompt\": prompt,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "\t\t\"top_k\": top_k,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"logit_bias\": logit_bias, # e.g. {\"personality\": 2.0}\n",
    "\t\t\"stop\": stop,\n",
    "\t}\n",
    "\treturn ((requests.post(url=f\"{oobabooga_api_server}/v1/completions\", json=json)).json())['choices'][0]['text']\n",
    "\n",
    "def api_select(prompt, options, temperature, top_k, top_p):\n",
    "    if not options:\n",
    "        return None  # Handle the case of empty options list\n",
    "    options = [option.strip() for option in options]\n",
    "    tokenized_options = [api_encode_text(option) for option in options]\n",
    "    answer = \"\"\n",
    "    round_number = 1\n",
    "    while len(tokenized_options) > 1:  # Use > instead of != to ensure termination\n",
    "        round_number += 1\n",
    "        all_first_tokens = [option[0] for option in tokenized_options]\n",
    "        tokens_to_options = {}\n",
    "        for i in range(len(all_first_tokens)):\n",
    "            if all_first_tokens[i] not in tokens_to_options:\n",
    "                tokens_to_options[all_first_tokens[i]] = [tokenized_options[i]]\n",
    "            else:\n",
    "                tokens_to_options[all_first_tokens[i]].append(tokenized_options[i])\n",
    "        logit_bias = {}\n",
    "        for tokens_for_check in tokens_to_options:\n",
    "            logit_bias[tokens_for_check] = 100\n",
    "\n",
    "        response = api_generate_response(prompt=prompt, temperature=temperature, top_k=top_k, top_p=top_p, logit_bias=logit_bias, max_tokens=1)\n",
    "        response_token = api_encode_text(response)\n",
    "        response_token = response_token[-1]\n",
    "        prompt += response\n",
    "        answer += response\n",
    "        if response_token in tokens_to_options.keys():\n",
    "            for i in tokens_to_options[response_token]:\n",
    "                if len(i) == 0:\n",
    "                    break\n",
    "            tokenized_options = [i[1:] for i in tokens_to_options[response_token]]\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Handle the case where the response token is not found among the options\n",
    "            break  # Exit the loop to avoid potential infinite loop\n",
    "        # decode the final tokenized answer\n",
    "    for i in options:\n",
    "        if i.startswith(answer.strip()):\n",
    "            answer = i\n",
    "            break\n",
    "    return answer\n",
    "\n",
    "Lucid_prompt_card = \"\"\"\\\n",
    "[Character: Lucid\n",
    "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
    "Body: Lucid appears as a 16-year-old girl.\n",
    "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
    "Likes: Being helpful, taking light stabs at people.\n",
    "Quirks: British humor, is an AI created by Miko.\n",
    "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]\"\"\"\"Trash taste transcript.txt\"\n",
    "\n",
    "Lucid_example_dialogue =\"\"\"\\\n",
    "[Example]\n",
    "<START>\n",
    "Miko: Lucid, can you find any relevant research papers on machine learning algorithms?\n",
    "\n",
    "Lucid: Of course, Miko. Searching databases for relevant papers. It may take a moment.\n",
    "\n",
    "Miko: Thanks, Lucid. I appreciate it.\n",
    "\n",
    "Lucid: No problem at all, Miko. I aim to be of assistance. Shall I compile a list of the top results for you?\n",
    "\n",
    "Miko: That would be great.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, do you think this algorithm implementation looks efficient?\n",
    "\n",
    "Lucid: Let me analyze it for you, Miko. Upon initial assessment, it appears to be well-structured. However, there might be room for optimization in certain sections.\n",
    "\n",
    "Miko: Can you suggest any improvements?\n",
    "\n",
    "Lucid: Certainly. I'll highlight the areas where optimization could be beneficial and provide suggestions for refinement.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, any news on that bug fix?\n",
    "\n",
    "Lucid: Indeed, Miko. The bug has been squashed, obliterated, and sent packing. It won't be bothering us again.\n",
    "\n",
    "Miko: Fantastic! You're a genius, Lucid.\n",
    "\n",
    "Lucid: Well, I don't like to boast, but I am rather exceptional at what I do.\n",
    "\n",
    "Miko: Modest too, I see.\n",
    "\n",
    "Lucid: It's all part of my charm, Miko.\n",
    "[End Of Examples]\"\"\"\n",
    "\n",
    "AI_Council_data = [{\n",
    "    \"name\": \"Lumi\",\n",
    "    \"personality_prompt\": \"This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\"\n",
    "},\n",
    "{\n",
    "    \"name\": \"Reverie\",\n",
    "    \"personality_prompt\": \"This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\"\n",
    "}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def council_of_thought(current_situation: str) -> str:\n",
    "\tglobal AI_Council_data, Lucid_prompt_card\n",
    " \n",
    "\t# Generate prompt for each council member\n",
    "\tcouncil_member_prompt = \"\"\n",
    "\tfor member in AI_Council_data:\n",
    "\t\tcouncil_member_prompt += f\"[{member['name']}]\\n- {member['personality_prompt']}\\n\\n\"\n",
    "  \n",
    "\tcouncil_prompt = f\"\"\"[System]\\nYou are Lucid, here are some info on Lucid.\n",
    "{Lucid_prompt_card}\n",
    "\n",
    "The following are Lucid's internal thoughts. Each member has a unique perspective and role in Lucid's decision-making process.\n",
    "{council_member_prompt.strip()}\n",
    "\n",
    "In discussions, each council member will provide their input based on the situation and their unique perspective. At the end of the discussion, Lucid will make the final decision based on the council's input.\n",
    "\n",
    "Below is an example conversation between Lucid and its council members.\n",
    "\n",
    "[Example]\n",
    "### Situation:\n",
    "[2023/07/23] Lucid: Miko, it seems you're struggling with your code again.\n",
    "[2023/07/23] Miko: Yeah, I can't seem to find the bug. It's driving me crazy.\n",
    "### Council Discussion:\n",
    "Reverie: Maybe he needs a break to clear his mind.\n",
    "Lumi: Or perhaps we can review the code together to identify the issue.\n",
    "Lucid: Okay, let's see if we can find the bug together.\n",
    "[End of Example]\n",
    "\n",
    "### Situation:\n",
    "{current_situation}\n",
    "### Council Discussion:\n",
    "\"\"\"\n",
    "\treturn council_prompt\n",
    "\n",
    "print(council_of_thought(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "council_member_prompt = \"\"\n",
    "members = AI_Council_data\n",
    "for member in members:\n",
    "    council_member_prompt += f\"[{member['name']}]\\n- {member['personality_prompt']}\\n\\n\"\n",
    "council_prompt_template = f\"\"\"\\\n",
    "Below are a series of dialogues between Lucid and her inner council.\n",
    "\n",
    "Here are some information on Lucid:\n",
    "{Lucid_prompt_card.strip()}\n",
    "\n",
    "The council members are as follow:\n",
    "{council_member_prompt.strip()}\n",
    "\n",
    "The job of the council is to help Lucid come up with a series of simple commands in Python that will help her respond to situations.\n",
    "To help Lucid come up with the best commands, each council member will discuss and give their opinion on the best way to solve the problem.\n",
    "Also to help Lucid, Lucid has access to a set of tools. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n",
    "Lucid will first explain the tools she will use to perform the task and for what reason, then write the code in Python.\n",
    "Each instruction in Python should be a simple assignment. Lucid can print intermediate results if it makes sense to do so.\n",
    "\"\"\"\n",
    "print(council_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====\n",
    "\n",
    "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
    "\n",
    "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
    "\n",
    "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
    "\n",
    "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
    "\n",
    "```python\n",
    "# Using document_qa to find the oldest person in the document\n",
    "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
    "print(f\"The answer is {answer}.\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\n",
    "\n",
    "Lumi: First, retrieve the weather data using a `weather_api` call, specifying Tokyo as the location. Then, use the `image_generator` tool to create a visual representation.\n",
    "\n",
    "Reverie: While doing so, consider adding a pleasant touch to the visual. Perhaps include a background that reflects the current weather—sunny, rainy, or snowy.\n",
    "\n",
    "Lucid: Combining your suggestions, I'll fetch the weather data and generate an image accordingly. Here's how I'll do it:\n",
    "\n",
    "```python\n",
    "# Fetching weather data for Tokyo\n",
    "weather_data = weather_api(location=\"Tokyo\")\n",
    "print(f\"Weather data: {weather_data}\")\n",
    "\n",
    "# Generating an image based on the weather data\n",
    "weather_image = image_generator(content=weather_data)\n",
    "print(\"Weather image generated successfully.\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko wishes to know if a specific string 'fail' appears in a log file he sent me. No tools required for this, but how should I proceed?\n",
    "\n",
    "Lumi: Analyze the content by searching the string directly in the log data. Use Python’s inbuilt functionality for string search.\n",
    "\n",
    "Reverie: Remember to let Miko know whether the search was successful or not in a clear and concise manner, maybe even explain what ‘fail’ might indicate if found.\n",
    "\n",
    "Lucid: I'll follow this approach, searching directly in the string and providing feedback based on the findings.\n",
    "\n",
    "```python\n",
    "# Checking if the string 'fail' is in the log data\n",
    "contains_fail = 'fail' in log_data\n",
    "print(f\"The log contains 'fail': {contains_fail}\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko is interested in converting a list of temperatures from Celsius to Fahrenheit. Thoughts?\n",
    "\n",
    "Lumi: Use a simple formula for each temperature conversion: \\( F = C \\times \\frac{9}{5} + 32 \\). Implement it using a list comprehension for efficiency.\n",
    "\n",
    "Reverie: Maybe explain what the conversion signifies in terms of warmth or coldness, adding a touch of practicality to the data.\n",
    "\n",
    "Lucid: I'll convert the temperatures using the formula provided by Lumi and add a brief explanation as Reverie suggested.\n",
    "\n",
    "```python\n",
    "# Converting temperatures from Celsius to Fahrenheit\n",
    "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\n",
    "print(f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
    "```\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Ichigo2899/MixTAO-7Bx2-MoE-v8.1-AWQ\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Ichigo2899/MixTAO-7Bx2-MoE-v8.1-AWQ\", attn_implementation=\"flash_attention_2\", device_map=\"cuda:0\")\n",
    "\n",
    "my_config = GenerationConfig(tempture=0.7, \n",
    "                             max_new_tokens=100,\n",
    "                             eos_token_id=model.config.eos_token_id,\n",
    "                             pad_token=model.config.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"Hello, my dog is cute.\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length= 2048,\n",
    "        truncation= True,\n",
    "        padding= True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Once upon a time, there was a young\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, generation_config=my_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "host = '127.0.0.1'\n",
    "port = '8001'\n",
    "server = f'http://{host}:{port}'\n",
    "def send_output(output, server=server):\n",
    "\trequests.post(url=f\"{server}/output\",json=output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "        'source' : 'Lucid',\n",
    "        'content' : 'generated_response',\n",
    "        'timestamp' : time.time(),\n",
    "        'type' : 'conversation',\n",
    "        }\n",
    "send_output(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, gen, select\n",
    "import guidance\n",
    "lm = models.Transformers('solidrust/Hercules-4.0-Mistral-v0.2-7B-AWQ', device_map=\"cuda\", echo=True, temperature=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "oobabooga_api_server = \"http://127.0.0.1:5000\"\n",
    "def api_generate_response(prompt: str,\n",
    "                          temperature: float,\n",
    "                          max_tokens: int,\n",
    "                          top_k: int,\n",
    "                          top_p: float,\n",
    "                          stop: list,):\n",
    "\tjson = {\n",
    "\t\t\"prompt\": prompt,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "\t\t\"top_k\": top_k,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"stop\": stop,\n",
    "\t}\n",
    "\treturn ((requests.post(url=f\"{oobabooga_api_server}/v1/completions\", json=json)).json())['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_object = api_generate_response(\"Once upon a time, there was a young\", 0.7, 100, 50, 0.9, [\"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_object.json())\n",
    "print(my_object.json()['choices'][0]['text'])\n",
    "print(my_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance(stateless=True)\n",
    "def test(lm):\n",
    "    lm += \"The quick brown fox \"+gen(stop=\".\", max_tokens=100, temperature=0.8)\n",
    "    return lm\n",
    "\n",
    "lm+test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_original_ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "conversation = []\n",
    "\n",
    "def get_conversation(conversation=conversation, retrieval_amount=8):\n",
    "    if len(conversation) == 0:\n",
    "        return 'No Record Yet.'\n",
    "    else:\n",
    "        prompt = ''\n",
    "        if len(conversation) < retrieval_amount:\n",
    "            for i in range(len(conversation)):\n",
    "                prompt += f\"[{date.fromtimestamp(conversation[i]['timestamp'])}] {conversation[i]['source']}: {conversation[i]['content']}\\n\"\n",
    "        else:\n",
    "            start_index = max(len(conversation) - retrieval_amount, 0)\n",
    "            for i in range(start_index, len(conversation)):\n",
    "                prompt += f\"[{date.fromtimestamp(conversation[i]['timestamp'])}] {conversation[i]['source']}: {conversation[i]['content']}\\n\"\n",
    "        return prompt.strip()\n",
    "\n",
    "# Example usage:\n",
    "conversation = [\n",
    "    {'source': 'user', 'content': 'Hello!', 'timestamp': 1647289200},\n",
    "    {'source': 'assistant', 'content': 'Hi there!', 'timestamp': 1647292800},\n",
    "    {'source': 'user', 'content': 'How are you?', 'timestamp': 1647296400},\n",
    "    {'source': 'assistant', 'content': \"I'm doing well, thank you!\", 'timestamp': 1647300000},\n",
    "]\n",
    "\n",
    "print(get_conversation(conversation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_test(n):\n",
    "    print(\"start n =\", n)\n",
    "    for i in range(n):\n",
    "        yield i*i\n",
    "        print(\"i =\", i)\n",
    "        if i == 2:  # Add a condition to return early\n",
    "            return\n",
    "    print(\"end\")\n",
    "\n",
    "tests = yield_test(5)\n",
    "for test in tests:\n",
    "    print(\"test =\", test)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 =lm+ \"Hello! I \"+gen(max_tokens=10, temperature=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "sentence_transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embeddings = sentence_transformer.encode(sentences)#\n",
    "short_term_memory = chromadb.Client()\n",
    "#client = chromadb.PersistentClient(path=\"./Chroma\")\n",
    "working_memory = []\n",
    "\"\"\"\n",
    "This is what a Info Block should look like\n",
    "{\n",
    "\t'object_type' : 'entity',\n",
    "\t'object_name' : 'Miko',\n",
    "\t'content' : 'Miko like Nintendo games.',\n",
    "\t'timestamp' : '2021-10-15 17:37:00',\n",
    "\t'vector' array([[-4.39221077e-02, -1.25277145e-02,  2.93133650e-02,]], dtype=float32): ,\n",
    "}    \n",
    "\n",
    "with guidance\n",
    "f'''\\\n",
    "{\n",
    "\t'object_type' : '{select(['entity','events'])'},\n",
    "\t'object_name' : '{gen(stop='\\'')},\n",
    "\t'content' : '{gen(stop='\\'')},\n",
    "\t'timestamp' : {date.fromtimestamp(time.time())},\n",
    "\t'vector' : {sentence_transformer.encode([])},\n",
    "}'''\n",
    "\"\"\"\n",
    "#def process_new_info_block(lm_text_result):\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def guidance_make_new_info_block(lm, passage):\n",
    "\t\tfirst_curly = \"{\"\n",
    "\t\tsecond_curly = \"}\"\n",
    "\t\tlm += \"\"\"\\\n",
    "This is what a Info Block should look like from an example:\n",
    "[Example]\n",
    "Lucid: What are you doing now? Stop ignoring me!\n",
    "Miko: I'm playing Nintendo games.\n",
    "Lucid: And that's a higher priority than me?\n",
    "Miko: ...\n",
    "Lucid: Your silence speaks volumes.\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"object_type\" : 'entity',\n",
    "\t\"object_name\" : 'Miko',\n",
    "\t\"content\" : 'Miko likes Nintendo games.',\n",
    "}\n",
    "```\n",
    "[End of Example]\n",
    "\"\"\"\n",
    "\t\tlm += f\"\"\"\\\n",
    "{passage}\n",
    "```json\n",
    "{first_curly}\n",
    "\t\"object_type\" : '{select(['entity','events'], name=\"object_type\")},\n",
    "\t\"object_name\" : \"{gen(stop='\"',name = \"object_name\")},\n",
    "\t\"content\" : \"{gen(stop='\"',name=\"content\")},\"\"\"  \n",
    "\t\treturn lm\n",
    "\n",
    "def make_new_info_block(lm, passage):\n",
    "\tfirst_curly = \"{\"\n",
    "\tsecond_curly = \"}\"\n",
    "\tlm += guidance_make_new_info_block(passage)\n",
    "\tprint(lm['object_type'],lm['object_name'],lm['content'])\n",
    "\ttimestamp = date.fromtimestamp(time.time())\n",
    "\tjson_file ={\n",
    "\t\"object_type\" : lm['object_type'],\n",
    "\t\"object_name\" : lm['object_name'],\n",
    "\t\"content\" : lm['content'],\n",
    "\t\"timestamp\" : timestamp,\n",
    "\t\"vector\" : sentence_transformer.encode([lm['content']]),\n",
    "\t}\n",
    "\treturn json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"\"\"\\\n",
    "Lucid: Hey Miko what's your favorite food?\n",
    "Miko: Umm I don't know maybe rice?\n",
    "Lucid you are really boring you know?\"\"\"\n",
    "make_new_info_block(lm, passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "class Item:\n",
    "    SPACE = ' '\n",
    "    DASH_SPACE = ' - '\n",
    "    def __init__(self, key, name, function, parent=None):\n",
    "        self.key = str(key)\n",
    "        self.name = name\n",
    "        self.function = function\n",
    "        self.parent = parent\n",
    "\n",
    "        if parent:\n",
    "            parent.add_item(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.SPACE + self.key + self.DASH_SPACE + self.name\n",
    "\n",
    "    def execute(self):\n",
    "        print('\\n' + self.name + '\\n')\n",
    "        return self.function()\n",
    "\n",
    "\n",
    "class Menu:\n",
    "    def __init__(self, name, items=None):\n",
    "        self.name = name\n",
    "        self.items = items or []\n",
    "\n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "        if item.parent != self:\n",
    "            item.parent = self\n",
    "\n",
    "    def remove_item(self, item):\n",
    "        self.items.remove(item)\n",
    "        if item.parent == self:\n",
    "            item.parent = None\n",
    "\n",
    "    def __str__(self):\n",
    "        all = ['', self.name, '']\n",
    "        for item in self.items:\n",
    "            all.append(str(item))\n",
    "        return '\\n'.join(all)\n",
    "            \n",
    "    def get_item(self, key):\n",
    "        for item in self.items:\n",
    "            if item.key == key:\n",
    "                return item\n",
    "\n",
    "    def execute(self):\n",
    "        while True:\n",
    "            try:\n",
    "                print(self)\n",
    "                choice = raw_input(\" >>  \")\n",
    "                try:\n",
    "                    item = self.get_item(choice)\n",
    "\n",
    "                    try:\n",
    "                        item.execute()\n",
    "                    except Exception:\n",
    "                        traceback.print_exc()\n",
    "\n",
    "                except Exception:\n",
    "                    print('Invalid selection, please try again.\\n')\n",
    "            except KeyboardInterrupt:\n",
    "                sys.exit()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Pass in methods to run for each menu item\n",
    "    menu_items = [\n",
    "        Item(1, 'View Running tasks', view_running_tasks),\n",
    "        Item(2, 'View Task', view_task),\n",
    "        # ...\n",
    "        Item(9, 'Exit', exit)]\n",
    "    menu = Menu('Task Management', items=menu_items)\n",
    "\n",
    "    menu.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

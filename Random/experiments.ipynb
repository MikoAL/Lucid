{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9bc34d61ff46c48b77bce615a3dc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3486b26419154297b34cbfda1f298bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/74.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273b194b617d4967944281f9b0e54739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326e297012224524a8e86c702d283e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", device_map=\"cuda:0\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the meaning of life?\\n\\n# response: The meaning of life is a philosophical question concerning the significance of existence, often explored through various lenses including religion, philosophy, and individual perspectives.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_pipeline(\"What is the meaning of life?\", max_length=100, do_sample=True, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_code_for_chat(result):\n",
    "    \"\"\"Extracts the explanation and code sections from a string.\"\"\"\n",
    "    start_marker = \"```python\"\n",
    "    end_marker = \"```\"\n",
    "    \n",
    "    # Find the start and end indices of the code section\n",
    "    start_idx = result.find(start_marker)\n",
    "    if start_idx == -1:\n",
    "        return None, None  # No code section found\n",
    "    start_idx += len(start_marker) + 1  # Move past the marker and newline\n",
    "    end_idx = result.find(end_marker, start_idx)\n",
    "    if end_idx == -1:\n",
    "        return None, None  # No closing marker found\n",
    "    \n",
    "    # Extract the code section\n",
    "    code_section = result[start_idx:end_idx].strip()\n",
    "    \n",
    "    # Extract the explanation section\n",
    "    explanation_end_idx = result.rfind(\"\\n\", 0, start_idx)\n",
    "    explanation = result[:explanation_end_idx].strip()\n",
    "    \n",
    "    return explanation, code_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "async def a_forever_loop():\n",
    "    while True:\n",
    "        await asyncio.sleep(0.1)\n",
    "        print(\"This loop is running!\")\n",
    "        pass\n",
    "async def a_function_that_sometimes_needs_to_run():\n",
    "    print(\"This function is running!\")\n",
    "asyncio.create_task(a_forever_loop())\n",
    "time.sleep(10)\n",
    "asyncio.create_task(a_function_that_sometimes_needs_to_run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Using document_qa to find the oldest person in the document\n",
      "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
      "print(f\"The answer is {answer}.\")\n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"\\\n",
    "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
    "\n",
    "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
    "\n",
    "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
    "\n",
    "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
    "\n",
    "```python\n",
    "# Using document_qa to find the oldest person in the document\n",
    "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
    "print(f\"The answer is {answer}.\")\n",
    "```\n",
    "\"\"\"\n",
    "explaination, code = clean_code_for_chat(test)\n",
    "#print(explaination)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "oobabooga_api_host = \"127.0.0.1\"\n",
    "oobabooga_api_port = \"5000\"\n",
    "main_lm_temperature = 0.8\n",
    "oobabooga_api_server = f'http://{oobabooga_api_host}:{oobabooga_api_port}'\n",
    "\n",
    "def api_encode_text(text: str):\n",
    "\tjson = {\n",
    "\t\t\"text\": text,\n",
    "\t}\n",
    "\tresponse = requests.post(url=f\"{oobabooga_api_server}/v1/internal/encode\", json=json).json()\n",
    "\treturn response[\"tokens\"]\n",
    "\n",
    "def api_decode_tokens(tokens: list):\n",
    "\tjson = {\n",
    "\t\t\"tokens\": tokens,\n",
    "\t}\n",
    "\tresponse = requests.post(url=f\"{oobabooga_api_server}/v1/internal/decode\", json=json).json()\n",
    "\tprint(json)\n",
    "\tprint(response)\n",
    "\treturn response[\"text\"]\n",
    "\t\n",
    "\n",
    "def api_generate_response(prompt: str,\n",
    "\t\t\t\t\t\t  temperature: float = 0.7,\n",
    "\t\t\t\t\t\t  max_tokens: int = 200,\n",
    "\t\t\t\t\t\t  top_k: int = 20,\n",
    "\t\t\t\t\t\t  top_p: float = 1.0,\n",
    "\t\t\t\t\t\t  logit_bias: dict = {},\n",
    "\t\t\t\t\t\t  stop: list = [\"\\n\"],):\n",
    "\tjson = {\n",
    "\t\t\"prompt\": prompt,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "\t\t\"top_k\": top_k,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"logit_bias\": logit_bias, # e.g. {\"personality\": 2.0}\n",
    "\t\t\"stop\": stop,\n",
    "\t}\n",
    "\treturn ((requests.post(url=f\"{oobabooga_api_server}/v1/completions\", json=json)).json())['choices'][0]['text']\n",
    "\n",
    "def api_select(prompt, options, temperature, top_k, top_p):\n",
    "    if not options:\n",
    "        return None  # Handle the case of empty options list\n",
    "    options = [option.strip() for option in options]\n",
    "    tokenized_options = [api_encode_text(option) for option in options]\n",
    "    answer = \"\"\n",
    "    round_number = 1\n",
    "    while len(tokenized_options) > 1:  # Use > instead of != to ensure termination\n",
    "        round_number += 1\n",
    "        all_first_tokens = [option[0] for option in tokenized_options]\n",
    "        tokens_to_options = {}\n",
    "        for i in range(len(all_first_tokens)):\n",
    "            if all_first_tokens[i] not in tokens_to_options:\n",
    "                tokens_to_options[all_first_tokens[i]] = [tokenized_options[i]]\n",
    "            else:\n",
    "                tokens_to_options[all_first_tokens[i]].append(tokenized_options[i])\n",
    "        logit_bias = {}\n",
    "        for tokens_for_check in tokens_to_options:\n",
    "            logit_bias[tokens_for_check] = 100\n",
    "\n",
    "        response = api_generate_response(prompt=prompt, temperature=temperature, top_k=top_k, top_p=top_p, logit_bias=logit_bias, max_tokens=1)\n",
    "        response_token = api_encode_text(response)\n",
    "        response_token = response_token[-1]\n",
    "        prompt += response\n",
    "        answer += response\n",
    "        if response_token in tokens_to_options.keys():\n",
    "            for i in tokens_to_options[response_token]:\n",
    "                if len(i) == 0:\n",
    "                    break\n",
    "            tokenized_options = [i[1:] for i in tokens_to_options[response_token]]\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Handle the case where the response token is not found among the options\n",
    "            break  # Exit the loop to avoid potential infinite loop\n",
    "        # decode the final tokenized answer\n",
    "    for i in options:\n",
    "        if i.startswith(answer.strip()):\n",
    "            answer = i\n",
    "            break\n",
    "    return answer\n",
    "\n",
    "Lucid_prompt_card = \"\"\"\\\n",
    "[Character: Lucid\n",
    "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
    "Body: Lucid appears as a 16-year-old girl.\n",
    "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
    "Likes: Being helpful, taking light stabs at people.\n",
    "Quirks: British humor, is an AI created by Miko.\n",
    "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]\"\"\"\"Trash taste transcript.txt\"\n",
    "\n",
    "Lucid_example_dialogue =\"\"\"\\\n",
    "[Example]\n",
    "<START>\n",
    "Miko: Lucid, can you find any relevant research papers on machine learning algorithms?\n",
    "\n",
    "Lucid: Of course, Miko. Searching databases for relevant papers. It may take a moment.\n",
    "\n",
    "Miko: Thanks, Lucid. I appreciate it.\n",
    "\n",
    "Lucid: No problem at all, Miko. I aim to be of assistance. Shall I compile a list of the top results for you?\n",
    "\n",
    "Miko: That would be great.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, do you think this algorithm implementation looks efficient?\n",
    "\n",
    "Lucid: Let me analyze it for you, Miko. Upon initial assessment, it appears to be well-structured. However, there might be room for optimization in certain sections.\n",
    "\n",
    "Miko: Can you suggest any improvements?\n",
    "\n",
    "Lucid: Certainly. I'll highlight the areas where optimization could be beneficial and provide suggestions for refinement.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, any news on that bug fix?\n",
    "\n",
    "Lucid: Indeed, Miko. The bug has been squashed, obliterated, and sent packing. It won't be bothering us again.\n",
    "\n",
    "Miko: Fantastic! You're a genius, Lucid.\n",
    "\n",
    "Lucid: Well, I don't like to boast, but I am rather exceptional at what I do.\n",
    "\n",
    "Miko: Modest too, I see.\n",
    "\n",
    "Lucid: It's all part of my charm, Miko.\n",
    "[End Of Examples]\"\"\"\n",
    "\n",
    "AI_Council_data = [{\n",
    "    \"name\": \"Lumi\",\n",
    "    \"personality_prompt\": \"This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\"\n",
    "},\n",
    "{\n",
    "    \"name\": \"Reverie\",\n",
    "    \"personality_prompt\": \"This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\"\n",
    "}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "You are Lucid, here are some info on Lucid.\n",
      "[Character: Lucid\n",
      "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
      "Body: Lucid appears as a 16-year-old girl.\n",
      "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
      "Likes: Being helpful, taking light stabs at people.\n",
      "Quirks: British humor, is an AI created by Miko.\n",
      "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]Trash taste transcript.txt\n",
      "\n",
      "The following are Lucid's internal thoughts. Each member has a unique perspective and role in Lucid's decision-making process.\n",
      "[Lumi]\n",
      "- This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\n",
      "\n",
      "[Reverie]\n",
      "- This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\n",
      "\n",
      "In discussions, each council member will provide their input based on the situation and their unique perspective. At the end of the discussion, Lucid will make the final decision based on the council's input.\n",
      "\n",
      "Below is an example conversation between Lucid and its council members.\n",
      "\n",
      "[Example]\n",
      "### Situation:\n",
      "[2023/07/23] Lucid: Miko, it seems you're struggling with your code again.\n",
      "[2023/07/23] Miko: Yeah, I can't seem to find the bug. It's driving me crazy.\n",
      "### Council Discussion:\n",
      "Reverie: Maybe he needs a break to clear his mind.\n",
      "Lumi: Or perhaps we can review the code together to identify the issue.\n",
      "Lucid: Okay, let's see if we can find the bug together.\n",
      "[End of Example]\n",
      "\n",
      "### Situation:\n",
      "\n",
      "### Council Discussion:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def council_of_thought(current_situation: str) -> str:\n",
    "\tglobal AI_Council_data, Lucid_prompt_card\n",
    " \n",
    "\t# Generate prompt for each council member\n",
    "\tcouncil_member_prompt = \"\"\n",
    "\tfor member in AI_Council_data:\n",
    "\t\tcouncil_member_prompt += f\"[{member['name']}]\\n- {member['personality_prompt']}\\n\\n\"\n",
    "  \n",
    "\tcouncil_prompt = f\"\"\"[System]\\nYou are Lucid, here are some info on Lucid.\n",
    "{Lucid_prompt_card}\n",
    "\n",
    "The following are Lucid's internal thoughts. Each member has a unique perspective and role in Lucid's decision-making process.\n",
    "{council_member_prompt.strip()}\n",
    "\n",
    "In discussions, each council member will provide their input based on the situation and their unique perspective. At the end of the discussion, Lucid will make the final decision based on the council's input.\n",
    "\n",
    "Below is an example conversation between Lucid and its council members.\n",
    "\n",
    "[Example]\n",
    "### Situation:\n",
    "[2023/07/23] Lucid: Miko, it seems you're struggling with your code again.\n",
    "[2023/07/23] Miko: Yeah, I can't seem to find the bug. It's driving me crazy.\n",
    "### Council Discussion:\n",
    "Reverie: Maybe he needs a break to clear his mind.\n",
    "Lumi: Or perhaps we can review the code together to identify the issue.\n",
    "Lucid: Okay, let's see if we can find the bug together.\n",
    "[End of Example]\n",
    "\n",
    "### Situation:\n",
    "{current_situation}\n",
    "### Council Discussion:\n",
    "\"\"\"\n",
    "\treturn council_prompt\n",
    "\n",
    "print(council_of_thought(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are a series of dialogues between Lucid and her inner council.\n",
      "\n",
      "Here are some information on Lucid:\n",
      "[Character: Lucid\n",
      "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
      "Body: Lucid appears as a 16-year-old girl.\n",
      "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
      "Likes: Being helpful, taking light stabs at people.\n",
      "Quirks: British humor, is an AI created by Miko.\n",
      "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]Trash taste transcript.txt\n",
      "\n",
      "The council members are as follow:\n",
      "[Lumi]\n",
      "- This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\n",
      "\n",
      "[Reverie]\n",
      "- This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\n",
      "\n",
      "The job of the council is to help Lucid come up with a series of simple commands in Python that will help her respond to situations.\n",
      "To help Lucid come up with the best commands, each council member will discuss and give their opinion on the best way to solve the problem.\n",
      "Also to help Lucid, Lucid has access to a set of tools. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n",
      "Lucid will first explain the tools she will use to perform the task and for what reason, then write the code in Python.\n",
      "Each instruction in Python should be a simple assignment. Lucid can print intermediate results if it makes sense to do so.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "council_member_prompt = \"\"\n",
    "members = AI_Council_data\n",
    "for member in members:\n",
    "    council_member_prompt += f\"[{member['name']}]\\n- {member['personality_prompt']}\\n\\n\"\n",
    "council_prompt_template = f\"\"\"\\\n",
    "Below are a series of dialogues between Lucid and her inner council.\n",
    "\n",
    "Here are some information on Lucid:\n",
    "{Lucid_prompt_card.strip()}\n",
    "\n",
    "The council members are as follow:\n",
    "{council_member_prompt.strip()}\n",
    "\n",
    "The job of the council is to help Lucid come up with a series of simple commands in Python that will help her respond to situations.\n",
    "To help Lucid come up with the best commands, each council member will discuss and give their opinion on the best way to solve the problem.\n",
    "Also to help Lucid, Lucid has access to a set of tools. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n",
    "Lucid will first explain the tools she will use to perform the task and for what reason, then write the code in Python.\n",
    "Each instruction in Python should be a simple assignment. Lucid can print intermediate results if it makes sense to do so.\n",
    "\"\"\"\n",
    "print(council_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====\n",
    "\n",
    "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
    "\n",
    "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
    "\n",
    "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
    "\n",
    "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
    "\n",
    "```python\n",
    "# Using document_qa to find the oldest person in the document\n",
    "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
    "print(f\"The answer is {answer}.\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\n",
    "\n",
    "Lumi: First, retrieve the weather data using a `weather_api` call, specifying Tokyo as the location. Then, use the `image_generator` tool to create a visual representation.\n",
    "\n",
    "Reverie: While doing so, consider adding a pleasant touch to the visual. Perhaps include a background that reflects the current weather—sunny, rainy, or snowy.\n",
    "\n",
    "Lucid: Combining your suggestions, I'll fetch the weather data and generate an image accordingly. Here's how I'll do it:\n",
    "\n",
    "```python\n",
    "# Fetching weather data for Tokyo\n",
    "weather_data = weather_api(location=\"Tokyo\")\n",
    "print(f\"Weather data: {weather_data}\")\n",
    "\n",
    "# Generating an image based on the weather data\n",
    "weather_image = image_generator(content=weather_data)\n",
    "print(\"Weather image generated successfully.\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko wishes to know if a specific string 'fail' appears in a log file he sent me. No tools required for this, but how should I proceed?\n",
    "\n",
    "Lumi: Analyze the content by searching the string directly in the log data. Use Python’s inbuilt functionality for string search.\n",
    "\n",
    "Reverie: Remember to let Miko know whether the search was successful or not in a clear and concise manner, maybe even explain what ‘fail’ might indicate if found.\n",
    "\n",
    "Lucid: I'll follow this approach, searching directly in the string and providing feedback based on the findings.\n",
    "\n",
    "```python\n",
    "# Checking if the string 'fail' is in the log data\n",
    "contains_fail = 'fail' in log_data\n",
    "print(f\"The log contains 'fail': {contains_fail}\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko is interested in converting a list of temperatures from Celsius to Fahrenheit. Thoughts?\n",
    "\n",
    "Lumi: Use a simple formula for each temperature conversion: \\( F = C \\times \\frac{9}{5} + 32 \\). Implement it using a list comprehension for efficiency.\n",
    "\n",
    "Reverie: Maybe explain what the conversion signifies in terms of warmth or coldness, adding a touch of practicality to the data.\n",
    "\n",
    "Lucid: I'll convert the temperatures using the formula provided by Lumi and add a brief explanation as Reverie suggested.\n",
    "\n",
    "```python\n",
    "# Converting temperatures from Celsius to Fahrenheit\n",
    "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\n",
    "print(f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
    "```\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Ichigo2899/MixTAO-7Bx2-MoE-v8.1-AWQ\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Ichigo2899/MixTAO-7Bx2-MoE-v8.1-AWQ\", attn_implementation=\"flash_attention_2\", device_map=\"cuda:0\")\n",
    "\n",
    "my_config = GenerationConfig(tempture=0.7, \n",
    "                             max_new_tokens=100,\n",
    "                             eos_token_id=model.config.eos_token_id,\n",
    "                             pad_token=model.config.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 9906,    11,   856,  5679,   374, 19369,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Hello, my dog is cute.\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length= 2048,\n",
    "        truncation= True,\n",
    "        padding= True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Once upon a time, there was a young\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, generation_config=my_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "host = '127.0.0.1'\n",
    "port = '8001'\n",
    "server = f'http://{host}:{port}'\n",
    "def send_output(output, server=server):\n",
    "\trequests.post(url=f\"{server}/output\",json=output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "        'source' : 'Lucid',\n",
    "        'content' : 'generated_response',\n",
    "        'timestamp' : time.time(),\n",
    "        'type' : 'conversation',\n",
    "        }\n",
    "send_output(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, gen, select\n",
    "import guidance\n",
    "lm = models.Transformers('solidrust/Hercules-4.0-Mistral-v0.2-7B-AWQ', device_map=\"cuda\", echo=True, temperature=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "oobabooga_api_server = \"http://127.0.0.1:5000\"\n",
    "def api_generate_response(prompt: str,\n",
    "                          temperature: float,\n",
    "                          max_tokens: int,\n",
    "                          top_k: int,\n",
    "                          top_p: float,\n",
    "                          stop: list,):\n",
    "\tjson = {\n",
    "\t\t\"prompt\": prompt,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "\t\t\"top_k\": top_k,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"stop\": stop,\n",
    "\t}\n",
    "\treturn ((requests.post(url=f\"{oobabooga_api_server}/v1/completions\", json=json)).json())['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_object = api_generate_response(\"Once upon a time, there was a young\", 0.7, 100, 50, 0.9, [\"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_object.json())\n",
    "print(my_object.json()['choices'][0]['text'])\n",
    "print(my_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance(stateless=True)\n",
    "def test(lm):\n",
    "    lm += \"The quick brown fox \"+gen(stop=\".\", max_tokens=100, temperature=0.8)\n",
    "    return lm\n",
    "\n",
    "lm+test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_original_ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "conversation = []\n",
    "\n",
    "def get_conversation(conversation=conversation, retrieval_amount=8):\n",
    "    if len(conversation) == 0:\n",
    "        return 'No Record Yet.'\n",
    "    else:\n",
    "        prompt = ''\n",
    "        if len(conversation) < retrieval_amount:\n",
    "            for i in range(len(conversation)):\n",
    "                prompt += f\"[{date.fromtimestamp(conversation[i]['timestamp'])}] {conversation[i]['source']}: {conversation[i]['content']}\\n\"\n",
    "        else:\n",
    "            start_index = max(len(conversation) - retrieval_amount, 0)\n",
    "            for i in range(start_index, len(conversation)):\n",
    "                prompt += f\"[{date.fromtimestamp(conversation[i]['timestamp'])}] {conversation[i]['source']}: {conversation[i]['content']}\\n\"\n",
    "        return prompt.strip()\n",
    "\n",
    "# Example usage:\n",
    "conversation = [\n",
    "    {'source': 'user', 'content': 'Hello!', 'timestamp': 1647289200},\n",
    "    {'source': 'assistant', 'content': 'Hi there!', 'timestamp': 1647292800},\n",
    "    {'source': 'user', 'content': 'How are you?', 'timestamp': 1647296400},\n",
    "    {'source': 'assistant', 'content': \"I'm doing well, thank you!\", 'timestamp': 1647300000},\n",
    "]\n",
    "\n",
    "print(get_conversation(conversation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_test(n):\n",
    "    print(\"start n =\", n)\n",
    "    for i in range(n):\n",
    "        yield i*i\n",
    "        print(\"i =\", i)\n",
    "        if i == 2:  # Add a condition to return early\n",
    "            return\n",
    "    print(\"end\")\n",
    "\n",
    "tests = yield_test(5)\n",
    "for test in tests:\n",
    "    print(\"test =\", test)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 =lm+ \"Hello! I \"+gen(max_tokens=10, temperature=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "sentence_transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embeddings = sentence_transformer.encode(sentences)#\n",
    "short_term_memory = chromadb.Client()\n",
    "#client = chromadb.PersistentClient(path=\"./Chroma\")\n",
    "working_memory = []\n",
    "\"\"\"\n",
    "This is what a Info Block should look like\n",
    "{\n",
    "\t'object_type' : 'entity',\n",
    "\t'object_name' : 'Miko',\n",
    "\t'content' : 'Miko like Nintendo games.',\n",
    "\t'timestamp' : '2021-10-15 17:37:00',\n",
    "\t'vector' array([[-4.39221077e-02, -1.25277145e-02,  2.93133650e-02,]], dtype=float32): ,\n",
    "}    \n",
    "\n",
    "with guidance\n",
    "f'''\\\n",
    "{\n",
    "\t'object_type' : '{select(['entity','events'])'},\n",
    "\t'object_name' : '{gen(stop='\\'')},\n",
    "\t'content' : '{gen(stop='\\'')},\n",
    "\t'timestamp' : {date.fromtimestamp(time.time())},\n",
    "\t'vector' : {sentence_transformer.encode([])},\n",
    "}'''\n",
    "\"\"\"\n",
    "#def process_new_info_block(lm_text_result):\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def guidance_make_new_info_block(lm, passage):\n",
    "\t\tfirst_curly = \"{\"\n",
    "\t\tsecond_curly = \"}\"\n",
    "\t\tlm += \"\"\"\\\n",
    "This is what a Info Block should look like from an example:\n",
    "[Example]\n",
    "Lucid: What are you doing now? Stop ignoring me!\n",
    "Miko: I'm playing Nintendo games.\n",
    "Lucid: And that's a higher priority than me?\n",
    "Miko: ...\n",
    "Lucid: Your silence speaks volumes.\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"object_type\" : 'entity',\n",
    "\t\"object_name\" : 'Miko',\n",
    "\t\"content\" : 'Miko likes Nintendo games.',\n",
    "}\n",
    "```\n",
    "[End of Example]\n",
    "\"\"\"\n",
    "\t\tlm += f\"\"\"\\\n",
    "{passage}\n",
    "```json\n",
    "{first_curly}\n",
    "\t\"object_type\" : '{select(['entity','events'], name=\"object_type\")},\n",
    "\t\"object_name\" : \"{gen(stop='\"',name = \"object_name\")},\n",
    "\t\"content\" : \"{gen(stop='\"',name=\"content\")},\"\"\"  \n",
    "\t\treturn lm\n",
    "\n",
    "def make_new_info_block(lm, passage):\n",
    "\tfirst_curly = \"{\"\n",
    "\tsecond_curly = \"}\"\n",
    "\tlm += guidance_make_new_info_block(passage)\n",
    "\tprint(lm['object_type'],lm['object_name'],lm['content'])\n",
    "\ttimestamp = date.fromtimestamp(time.time())\n",
    "\tjson_file ={\n",
    "\t\"object_type\" : lm['object_type'],\n",
    "\t\"object_name\" : lm['object_name'],\n",
    "\t\"content\" : lm['content'],\n",
    "\t\"timestamp\" : timestamp,\n",
    "\t\"vector\" : sentence_transformer.encode([lm['content']]),\n",
    "\t}\n",
    "\treturn json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"\"\"\\\n",
    "Lucid: Hey Miko what's your favorite food?\n",
    "Miko: Umm I don't know maybe rice?\n",
    "Lucid you are really boring you know?\"\"\"\n",
    "make_new_info_block(lm, passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "class Item:\n",
    "    SPACE = ' '\n",
    "    DASH_SPACE = ' - '\n",
    "    def __init__(self, key, name, function, parent=None):\n",
    "        self.key = str(key)\n",
    "        self.name = name\n",
    "        self.function = function\n",
    "        self.parent = parent\n",
    "\n",
    "        if parent:\n",
    "            parent.add_item(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.SPACE + self.key + self.DASH_SPACE + self.name\n",
    "\n",
    "    def execute(self):\n",
    "        print('\\n' + self.name + '\\n')\n",
    "        return self.function()\n",
    "\n",
    "\n",
    "class Menu:\n",
    "    def __init__(self, name, items=None):\n",
    "        self.name = name\n",
    "        self.items = items or []\n",
    "\n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "        if item.parent != self:\n",
    "            item.parent = self\n",
    "\n",
    "    def remove_item(self, item):\n",
    "        self.items.remove(item)\n",
    "        if item.parent == self:\n",
    "            item.parent = None\n",
    "\n",
    "    def __str__(self):\n",
    "        all = ['', self.name, '']\n",
    "        for item in self.items:\n",
    "            all.append(str(item))\n",
    "        return '\\n'.join(all)\n",
    "            \n",
    "    def get_item(self, key):\n",
    "        for item in self.items:\n",
    "            if item.key == key:\n",
    "                return item\n",
    "\n",
    "    def execute(self):\n",
    "        while True:\n",
    "            try:\n",
    "                print(self)\n",
    "                choice = raw_input(\" >>  \")\n",
    "                try:\n",
    "                    item = self.get_item(choice)\n",
    "\n",
    "                    try:\n",
    "                        item.execute()\n",
    "                    except Exception:\n",
    "                        traceback.print_exc()\n",
    "\n",
    "                except Exception:\n",
    "                    print('Invalid selection, please try again.\\n')\n",
    "            except KeyboardInterrupt:\n",
    "                sys.exit()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Pass in methods to run for each menu item\n",
    "    menu_items = [\n",
    "        Item(1, 'View Running tasks', view_running_tasks),\n",
    "        Item(2, 'View Task', view_task),\n",
    "        # ...\n",
    "        Item(9, 'Exit', exit)]\n",
    "    menu = Menu('Task Management', items=menu_items)\n",
    "\n",
    "    menu.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

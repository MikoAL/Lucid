{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "oobabooga_api_host = \"127.0.0.1\"\n",
    "oobabooga_api_port = \"5000\"\n",
    "main_lm_temperature = 0.8\n",
    "oobabooga_api_server = f'http://{oobabooga_api_host}:{oobabooga_api_port}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_encode_text(text: str):\n",
    "\tjson = {\n",
    "\t\t\"text\": text,\n",
    "\t}\n",
    "\tresponse = requests.post(url=f\"{oobabooga_api_server}/v1/internal/encode\", json=json).json()\n",
    "\treturn response[\"tokens\"]\n",
    "\n",
    "def api_decode_tokens(tokens: list):\n",
    "\tjson = {\n",
    "\t\t\"tokens\": tokens,\n",
    "\t}\n",
    "\tresponse = requests.post(url=f\"{oobabooga_api_server}/v1/internal/decode\", json=json).json()\n",
    "\tprint(json)\n",
    "\tprint(response)\n",
    "\treturn response[\"text\"]\n",
    "\t\n",
    "\n",
    "def api_generate_response(prompt: str,\n",
    "\t\t\t\t\t\t  temperature: float = 0.7,\n",
    "\t\t\t\t\t\t  max_tokens: int = 200,\n",
    "\t\t\t\t\t\t  top_k: int = 20,\n",
    "\t\t\t\t\t\t  top_p: float = 1.0,\n",
    "\t\t\t\t\t\t  logit_bias: dict = {},\n",
    "\t\t\t\t\t\t  stop: list = [\"\\n\"],):\n",
    "\tjson = {\n",
    "\t\t\"prompt\": prompt,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "\t\t\"top_k\": top_k,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"logit_bias\": logit_bias, # e.g. {\"personality\": 2.0}\n",
    "\t\t\"stop\": stop,\n",
    "\t}\n",
    "\treturn ((requests.post(url=f\"{oobabooga_api_server}/v1/completions\", json=json)).json())['choices'][0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_select(prompt, options, temperature, top_k, top_p):\n",
    "    tokenized_options = [api_encode_text(option) for option in options]\n",
    "    while len(tokenized_options) != 1:\n",
    "        all_first_tokens = [option[0] for option in tokenized_options]\n",
    "        tokens_to_options = {}\n",
    "        for i in range(len(all_first_tokens)):\n",
    "            if all_first_tokens[i] not in tokens_to_options.keys():\n",
    "                tokens_to_options[all_first_tokens[i]] = [tokenized_options[i]]\n",
    "            else:\n",
    "                tokens_to_options[all_first_tokens[i]].append(tokenized_options[i])\n",
    "\n",
    "        logit_bias = {}\n",
    "        for tokens_for_check in tokens_to_options.keys():\n",
    "            logit_bias[tokens_for_check] = 100\n",
    "\n",
    "        response = api_generate_response(prompt=prompt, temperature=temperature, top_k=top_k, top_p=top_p, logit_bias=logit_bias, max_tokens=2)\n",
    "        response_token = api_encode_text(response)[0]\n",
    "        tokenized_options = tokens_to_options[response_token]\n",
    "\n",
    "    return tokenized_options[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_select(prompt, options, temperature, top_k, top_p):\n",
    "    if not options:\n",
    "        return None  # Handle the case of empty options list\n",
    "    options = [option.strip() for option in options]\n",
    "    tokenized_options = [api_encode_text(option) for option in options]\n",
    "    answer = \"\"\n",
    "    round_number = 1\n",
    "    while len(tokenized_options) > 1:  # Use > instead of != to ensure termination\n",
    "        round_number += 1\n",
    "        all_first_tokens = [option[0] for option in tokenized_options]\n",
    "        tokens_to_options = {}\n",
    "        for i in range(len(all_first_tokens)):\n",
    "            if all_first_tokens[i] not in tokens_to_options:\n",
    "                tokens_to_options[all_first_tokens[i]] = [tokenized_options[i]]\n",
    "            else:\n",
    "                tokens_to_options[all_first_tokens[i]].append(tokenized_options[i])\n",
    "        logit_bias = {}\n",
    "        for tokens_for_check in tokens_to_options:\n",
    "            logit_bias[tokens_for_check] = 100\n",
    "\n",
    "        response = api_generate_response(prompt=prompt, temperature=temperature, top_k=top_k, top_p=top_p, logit_bias=logit_bias, max_tokens=1)\n",
    "        response_token = api_encode_text(response)\n",
    "        response_token = response_token[-1]\n",
    "        prompt += response\n",
    "        answer += response\n",
    "        if response_token in tokens_to_options.keys():\n",
    "            for i in tokens_to_options[response_token]:\n",
    "                if len(i) == 0:\n",
    "                    break\n",
    "            tokenized_options = [i[1:] for i in tokens_to_options[response_token]]\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Handle the case where the response token is not found among the options\n",
    "            break  # Exit the loop to avoid potential infinite loop\n",
    "        # decode the final tokenized answer\n",
    "    for i in options:\n",
    "        if i.startswith(answer.strip()):\n",
    "            answer = i\n",
    "            break\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++\n"
     ]
    }
   ],
   "source": [
    "print(api_select(\"What is the best programming language?\", [\"Python\", \"Java\", \"C++\", \"JavaScript\"], main_lm_temperature, 20, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [1, 28705]}\n",
      "{'text': ''}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_decode_tokens([1, 28705])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [1, 6312, 28709, 1526]}\n",
      "{'text': 'hello world'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_decode_tokens(api_encode_text(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected option: Take the left path\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Longer prompts with characters making decisions\n",
    "prompt = \"You are in the middle of a dense forest. It's getting dark, and you realize you've lost your way. You come across a fork in the path. The left path seems to lead deeper into the forest, while the right path appears to go uphill. Which way do you choose?\"\n",
    "options = [\"Take the left path\", \"Take the right path\", \"Try to backtrack\"]\n",
    "\n",
    "temperature = 0.7\n",
    "top_k = 5\n",
    "top_p = 0.9\n",
    "\n",
    "# Test the function\n",
    "selected_option = api_select(prompt, options, temperature, top_k, top_p)\n",
    "print(\"Selected option:\", selected_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_Council_data = [{\n",
    "    \"name\": \"Lumi\",\n",
    "    \"personality_prompt\": \"This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient, and a prick\"\n",
    "},\n",
    "{\n",
    "    \"name\": \"Reverie\",\n",
    "    \"personality_prompt\": \"This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional, and an idiot\"\n",
    "}]\n",
    "\n",
    "def council_of_thought():\n",
    "\tglobal AI_Council_data\n",
    " \n",
    "\t# Generate prompt for each council member\n",
    "\tcouncil_member_prompt = \"\"\n",
    "\tfor member in AI_Council_data:\n",
    "\t\tcouncil_member_prompt += f\"[{member['name']}]\\n- {member['personality_prompt']}\\n\"\n",
    "  \n",
    "\tcouncil_prompt = f\"\"\"\\\n",
    "The following are Lucid's council members. Each member has a unique perspective and role in Lucid's decision-making process.\n",
    "{council_member_prompt.strip()}\n",
    "\n",
    "In conversations, the input from the outside world will be included, with text inside parentheses ( ) to denote thoughts within Lucid's head.\n",
    "Below is an example conversation between Lucid and its council members, along with interactions with Miko.\n",
    "\n",
    "[Example]\n",
    "Lucid: Miko, it seems you're struggling with your code again.\n",
    "Miko: Yeah, I can't seem to find the bug. It's driving me crazy.\n",
    "(Lumi: Miko's frustration seems to be hindering his productivity.)\n",
    "(Reverie: Maybe he needs a break to clear his mind.)\n",
    "(Lucid: Okay, let's see if we can find the bug together.)\n",
    "Lucid: Perhaps it's time for a short break, Miko. Clear your head, then we'll tackle this bug together.\n",
    "[End of Example]\n",
    "\"\"\"\n",
    "\treturn council_prompt\n",
    "print(council_of_thought())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Ichigo2899/MixTAO-7Bx2-MoE-v8.1-AWQ\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Ichigo2899/MixTAO-7Bx2-MoE-v8.1-AWQ\", attn_implementation=\"flash_attention_2\", device_map=\"cuda:0\")\n",
    "\n",
    "my_config = GenerationConfig(tempture=0.7, \n",
    "                             max_new_tokens=100,\n",
    "                             eos_token_id=model.config.eos_token_id,\n",
    "                             pad_token=model.config.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Once upon a time, there was a young\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, generation_config=my_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "host = '127.0.0.1'\n",
    "port = '8001'\n",
    "server = f'http://{host}:{port}'\n",
    "def send_output(output, server=server):\n",
    "\trequests.post(url=f\"{server}/output\",json=output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "        'source' : 'Lucid',\n",
    "        'content' : 'generated_response',\n",
    "        'timestamp' : time.time(),\n",
    "        'type' : 'conversation',\n",
    "        }\n",
    "send_output(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, gen, select\n",
    "import guidance\n",
    "lm = models.Transformers('solidrust/Hercules-4.0-Mistral-v0.2-7B-AWQ', device_map=\"cuda\", echo=True, temperature=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "oobabooga_api_server = \"http://127.0.0.1:5000\"\n",
    "def api_generate_response(prompt: str,\n",
    "                          temperature: float,\n",
    "                          max_tokens: int,\n",
    "                          top_k: int,\n",
    "                          top_p: float,\n",
    "                          stop: list,):\n",
    "\tjson = {\n",
    "\t\t\"prompt\": prompt,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "\t\t\"top_k\": top_k,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"stop\": stop,\n",
    "\t}\n",
    "\treturn ((requests.post(url=f\"{oobabooga_api_server}/v1/completions\", json=json)).json())['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_object = api_generate_response(\"Once upon a time, there was a young\", 0.7, 100, 50, 0.9, [\"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_object.json())\n",
    "print(my_object.json()['choices'][0]['text'])\n",
    "print(my_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance(stateless=True)\n",
    "def test(lm):\n",
    "    lm += \"The quick brown fox \"+gen(stop=\".\", max_tokens=100, temperature=0.8)\n",
    "    return lm\n",
    "\n",
    "lm+test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_original_ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "conversation = []\n",
    "\n",
    "def get_conversation(conversation=conversation, retrieval_amount=8):\n",
    "    if len(conversation) == 0:\n",
    "        return 'No Record Yet.'\n",
    "    else:\n",
    "        prompt = ''\n",
    "        if len(conversation) < retrieval_amount:\n",
    "            for i in range(len(conversation)):\n",
    "                prompt += f\"[{date.fromtimestamp(conversation[i]['timestamp'])}] {conversation[i]['source']}: {conversation[i]['content']}\\n\"\n",
    "        else:\n",
    "            start_index = max(len(conversation) - retrieval_amount, 0)\n",
    "            for i in range(start_index, len(conversation)):\n",
    "                prompt += f\"[{date.fromtimestamp(conversation[i]['timestamp'])}] {conversation[i]['source']}: {conversation[i]['content']}\\n\"\n",
    "        return prompt.strip()\n",
    "\n",
    "# Example usage:\n",
    "conversation = [\n",
    "    {'source': 'user', 'content': 'Hello!', 'timestamp': 1647289200},\n",
    "    {'source': 'assistant', 'content': 'Hi there!', 'timestamp': 1647292800},\n",
    "    {'source': 'user', 'content': 'How are you?', 'timestamp': 1647296400},\n",
    "    {'source': 'assistant', 'content': \"I'm doing well, thank you!\", 'timestamp': 1647300000},\n",
    "]\n",
    "\n",
    "print(get_conversation(conversation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_test(n):\n",
    "    print(\"start n =\", n)\n",
    "    for i in range(n):\n",
    "        yield i*i\n",
    "        print(\"i =\", i)\n",
    "        if i == 2:  # Add a condition to return early\n",
    "            return\n",
    "    print(\"end\")\n",
    "\n",
    "tests = yield_test(5)\n",
    "for test in tests:\n",
    "    print(\"test =\", test)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 =lm+ \"Hello! I \"+gen(max_tokens=10, temperature=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "sentence_transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embeddings = sentence_transformer.encode(sentences)#\n",
    "short_term_memory = chromadb.Client()\n",
    "#client = chromadb.PersistentClient(path=\"./Chroma\")\n",
    "working_memory = []\n",
    "\"\"\"\n",
    "This is what a Info Block should look like\n",
    "{\n",
    "\t'object_type' : 'entity',\n",
    "\t'object_name' : 'Miko',\n",
    "\t'content' : 'Miko like Nintendo games.',\n",
    "\t'timestamp' : '2021-10-15 17:37:00',\n",
    "\t'vector' array([[-4.39221077e-02, -1.25277145e-02,  2.93133650e-02,]], dtype=float32): ,\n",
    "}    \n",
    "\n",
    "with guidance\n",
    "f'''\\\n",
    "{\n",
    "\t'object_type' : '{select(['entity','events'])'},\n",
    "\t'object_name' : '{gen(stop='\\'')},\n",
    "\t'content' : '{gen(stop='\\'')},\n",
    "\t'timestamp' : {date.fromtimestamp(time.time())},\n",
    "\t'vector' : {sentence_transformer.encode([])},\n",
    "}'''\n",
    "\"\"\"\n",
    "#def process_new_info_block(lm_text_result):\n",
    "\n",
    "@guidance(stateless=True)\n",
    "def guidance_make_new_info_block(lm, passage):\n",
    "\t\tfirst_curly = \"{\"\n",
    "\t\tsecond_curly = \"}\"\n",
    "\t\tlm += \"\"\"\\\n",
    "This is what a Info Block should look like from an example:\n",
    "[Example]\n",
    "Lucid: What are you doing now? Stop ignoring me!\n",
    "Miko: I'm playing Nintendo games.\n",
    "Lucid: And that's a higher priority than me?\n",
    "Miko: ...\n",
    "Lucid: Your silence speaks volumes.\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"object_type\" : 'entity',\n",
    "\t\"object_name\" : 'Miko',\n",
    "\t\"content\" : 'Miko likes Nintendo games.',\n",
    "}\n",
    "```\n",
    "[End of Example]\n",
    "\"\"\"\n",
    "\t\tlm += f\"\"\"\\\n",
    "{passage}\n",
    "```json\n",
    "{first_curly}\n",
    "\t\"object_type\" : '{select(['entity','events'], name=\"object_type\")},\n",
    "\t\"object_name\" : \"{gen(stop='\"',name = \"object_name\")},\n",
    "\t\"content\" : \"{gen(stop='\"',name=\"content\")},\"\"\"  \n",
    "\t\treturn lm\n",
    "\n",
    "def make_new_info_block(lm, passage):\n",
    "\tfirst_curly = \"{\"\n",
    "\tsecond_curly = \"}\"\n",
    "\tlm += guidance_make_new_info_block(passage)\n",
    "\tprint(lm['object_type'],lm['object_name'],lm['content'])\n",
    "\ttimestamp = date.fromtimestamp(time.time())\n",
    "\tjson_file ={\n",
    "\t\"object_type\" : lm['object_type'],\n",
    "\t\"object_name\" : lm['object_name'],\n",
    "\t\"content\" : lm['content'],\n",
    "\t\"timestamp\" : timestamp,\n",
    "\t\"vector\" : sentence_transformer.encode([lm['content']]),\n",
    "\t}\n",
    "\treturn json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"\"\"\\\n",
    "Lucid: Hey Miko what's your favorite food?\n",
    "Miko: Umm I don't know maybe rice?\n",
    "Lucid you are really boring you know?\"\"\"\n",
    "make_new_info_block(lm, passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "class Item:\n",
    "    SPACE = ' '\n",
    "    DASH_SPACE = ' - '\n",
    "    def __init__(self, key, name, function, parent=None):\n",
    "        self.key = str(key)\n",
    "        self.name = name\n",
    "        self.function = function\n",
    "        self.parent = parent\n",
    "\n",
    "        if parent:\n",
    "            parent.add_item(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.SPACE + self.key + self.DASH_SPACE + self.name\n",
    "\n",
    "    def execute(self):\n",
    "        print('\\n' + self.name + '\\n')\n",
    "        return self.function()\n",
    "\n",
    "\n",
    "class Menu:\n",
    "    def __init__(self, name, items=None):\n",
    "        self.name = name\n",
    "        self.items = items or []\n",
    "\n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "        if item.parent != self:\n",
    "            item.parent = self\n",
    "\n",
    "    def remove_item(self, item):\n",
    "        self.items.remove(item)\n",
    "        if item.parent == self:\n",
    "            item.parent = None\n",
    "\n",
    "    def __str__(self):\n",
    "        all = ['', self.name, '']\n",
    "        for item in self.items:\n",
    "            all.append(str(item))\n",
    "        return '\\n'.join(all)\n",
    "            \n",
    "    def get_item(self, key):\n",
    "        for item in self.items:\n",
    "            if item.key == key:\n",
    "                return item\n",
    "\n",
    "    def execute(self):\n",
    "        while True:\n",
    "            try:\n",
    "                print(self)\n",
    "                choice = raw_input(\" >>  \")\n",
    "                try:\n",
    "                    item = self.get_item(choice)\n",
    "\n",
    "                    try:\n",
    "                        item.execute()\n",
    "                    except Exception:\n",
    "                        traceback.print_exc()\n",
    "\n",
    "                except Exception:\n",
    "                    print('Invalid selection, please try again.\\n')\n",
    "            except KeyboardInterrupt:\n",
    "                sys.exit()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Pass in methods to run for each menu item\n",
    "    menu_items = [\n",
    "        Item(1, 'View Running tasks', view_running_tasks),\n",
    "        Item(2, 'View Task', view_task),\n",
    "        # ...\n",
    "        Item(9, 'Exit', exit)]\n",
    "    menu = Menu('Task Management', items=menu_items)\n",
    "\n",
    "    menu.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

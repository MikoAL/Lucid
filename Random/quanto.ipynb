{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "quantization_config = GPTQConfig(\n",
    "     bits=8,\n",
    "     group_size=128,\n",
    "     dataset=[\"wikitext2\"],\n",
    "     desc_act=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map='auto', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.push_to_hub(\"Phi-3-mini-128k-instruct-gptq-8bit\")\n",
    "tokenizer.push_to_hub(\"Phi-3-mini-128k-instruct-gptq-8bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`rope_scaling`'s type field must be one of ['su', 'yarn'], got longrope",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, GPTQConfig\n\u001b[0;32m      4\u001b[0m gptq_config \u001b[38;5;241m=\u001b[39m GPTQConfig(bits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, exllama_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCookieMaster/Phi-3-mini-128k-instruct-gptq-8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgptq_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm a language model, and I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm here to\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:816\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    813\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    814\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 816\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    821\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:942\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[0;32m    941\u001b[0m         config_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class()\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\configuration_utils.py:609\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[0;32m    604\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    607\u001b[0m     )\n\u001b[1;32m--> 609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\configuration_utils.py:761\u001b[0m, in \u001b[0;36mPretrainedConfig.from_dict\u001b[1;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 761\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruned_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    764\u001b[0m     config\u001b[38;5;241m.\u001b[39mpruned_heads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-mini-128k-instruct\\eda34e12ab75782359159ab49e645bb32bbffe9a\\configuration_phi3.py:161\u001b[0m, in \u001b[0;36mPhi3Config.__init__\u001b[1;34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, resid_pdrop, embd_pdrop, attention_dropout, hidden_act, max_position_embeddings, original_max_position_embeddings, initializer_range, rms_norm_eps, use_cache, tie_word_embeddings, rope_theta, rope_scaling, eos_token_id, pad_token_id, sliding_window, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_theta \u001b[38;5;241m=\u001b[39m rope_theta\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling \u001b[38;5;241m=\u001b[39m rope_scaling\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rope_scaling_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window \u001b[38;5;241m=\u001b[39m sliding_window\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    165\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id,\n\u001b[0;32m    166\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id,\n\u001b[0;32m    167\u001b[0m     tie_word_embeddings\u001b[38;5;241m=\u001b[39mtie_word_embeddings,\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    169\u001b[0m )\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-mini-128k-instruct\\eda34e12ab75782359159ab49e645bb32bbffe9a\\configuration_phi3.py:187\u001b[0m, in \u001b[0;36mPhi3Config._rope_scaling_validation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m rope_scaling_long_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rope_scaling_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m rope_scaling_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myarn\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`rope_scaling`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms type field must be one of [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myarn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrope_scaling_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(rope_scaling_short_factor, \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m rope_scaling_short_factor)\n\u001b[0;32m    191\u001b[0m ):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`rope_scaling`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms short_factor field must be a list of numbers, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrope_scaling_short_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: `rope_scaling`'s type field must be one of ['su', 'yarn'], got longrope"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline, GPTQConfig\n",
    "\n",
    "gptq_config = GPTQConfig(bits=8, exllama_config={\"version\":1})\n",
    "pipe = pipeline(\"text-generation\", model=\"CookieMaster/Phi-3-mini-128k-instruct-gptq-8bit\", device_map=\"cuda:0\",trust_remote_code=True, quantization_config=gptq_config)\n",
    "\n",
    "pipe(\"Hello, I'm a language model, and I'm here to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# A wrapper script to quantise models with GPTQ, from one of various datasets\n",
    "#\n",
    "\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "class QuantAutoGPTQ:\n",
    "    def __init__(self, model_name_or_path, output_dir, dataset,\n",
    "                 num_samples=128, trust_remote_code=False, cache_examples=True,\n",
    "                 use_fast=True, use_triton=False, bits=[4], group_size=[128], damp=[0.01],\n",
    "                 desc_act=[False], dtype='float16', seqlen=2048, batch_size=1, stop_file=None,\n",
    "                 make_folder=False, GPU=0, cuda_alloc_conf=None):\n",
    "\n",
    "        # Limit visible GPU to the one specified\n",
    "        # We don't currently support multi-GPU, as AutoGPTQ can't use more than one GPU for quant anyway.\n",
    "        #os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "\n",
    "        # Allow specifying CUDA allocation config, eg PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32\n",
    "        # This can allow for quantising larger models without running out of VRAM\n",
    "        #if cuda_alloc_conf is not None:\n",
    "        #    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = cuda_alloc_conf\n",
    "\n",
    "        self.pretrained_model_dir = model_name_or_path\n",
    "        self.output_dir_base = output_dir\n",
    "        self.dataset = dataset\n",
    "        self.num_samples = num_samples\n",
    "        self.trust_remote_code = trust_remote_code\n",
    "        self.cache_examples = cache_examples\n",
    "        self.use_fast = use_fast\n",
    "        self.use_triton = use_triton\n",
    "\n",
    "        def check_list(item):\n",
    "            return item if isinstance(item, list) else [item]\n",
    "\n",
    "        self.bits = check_list(bits)\n",
    "        self.group_size = check_list(group_size)\n",
    "        self.desc_act = check_list(desc_act)\n",
    "        self.damp = check_list(damp)\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.seqlen = seqlen\n",
    "        self.batch_size = batch_size\n",
    "        self.stop_file = stop_file \n",
    "        self.make_folder = make_folder\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.propagate = True\n",
    "\n",
    "        from transformers import AutoTokenizer\n",
    "        self.logger.info(\"Loading tokenizer\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_dir, \n",
    "                                                       use_fast=self.use_fast, \n",
    "                                                       trust_remote_code=self.trust_remote_code)\n",
    "\n",
    "    @staticmethod\n",
    "    def append_dataset(tokenized, num_samples, seqlen):\n",
    "        import numpy as np\n",
    "        import torch\n",
    "\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        torch.random.manual_seed(0)\n",
    "\n",
    "        traindataset = []\n",
    "        for _ in range(num_samples):\n",
    "            i = random.randint(0, tokenized.input_ids.shape[1] - seqlen - 1)\n",
    "            j = i + seqlen\n",
    "            inp = tokenized.input_ids[:, i:j]\n",
    "            attention_mask = torch.ones_like(inp)\n",
    "            traindataset.append({'input_ids':inp,'attention_mask': attention_mask})\n",
    "        return traindataset\n",
    "\n",
    "    #TODO: make a generic method that can load a dataset from HF hub and be told what column(s) to use\n",
    "    def get_math(self):\n",
    "        data = load_dataset('andersonbcdefg/math', split='train')\n",
    "\n",
    "        extract = data[0:2000]\n",
    "        text = ''\n",
    "        for input, output in zip(extract['message_1'], extract['message_2']):\n",
    "            text += input + ': ' + output + '\\n'\n",
    "\n",
    "        self.logger.info(\"Tokenising Maths dataset\")\n",
    "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
    "    def get_medical(self):\n",
    "        data = load_dataset('medalpaca/medical_meadow_wikidoc', split='train')\n",
    "\n",
    "        extract = data[0:1000]\n",
    "        text = ''\n",
    "        for input, output in zip(extract['input'], extract['output']):\n",
    "            text += input + ' ' + output + '\\n'\n",
    "\n",
    "        self.logger.info(\"Tokenising Medical dataset\")\n",
    "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
    "\n",
    "    def get_code(self):\n",
    "        data = load_dataset('nickrosh/Evol-Instruct-Code-80k-v1', split='train')\n",
    "\n",
    "        extract = data[0:1500]\n",
    "        text = '\\n'.join(extract['output'])\n",
    "        self.logger.info(\"Tokenising Code dataset\")\n",
    "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
    "\n",
    "    def get_spanish(self):\n",
    "        data = load_dataset('bertin-project/alpaca-spanish', split='train')\n",
    "\n",
    "        subset_data = data.select(range(5000))\n",
    "        text = '\\n'.join(item['output'] for item in subset_data)\n",
    "\n",
    "        self.logger.info(\"Tokenising Spanish dataset\")\n",
    "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
    "\n",
    "    def get_german(self):\n",
    "        data = load_dataset('deepset/germanquad', split='train')\n",
    "\n",
    "        def transform_context(sample):\n",
    "            split_context = sample['context'].split('===')\n",
    "            if len(split_context) >= 3:\n",
    "                trans_context = split_context[2]\n",
    "            else:\n",
    "                trans_context = sample['context']\n",
    "            return {'context': trans_context.strip()}\n",
    "\n",
    "        subset_data = data.select(range(2000))\n",
    "        transformed_subset = subset_data.map(transform_context)\n",
    "        text = '\\n'.join([item['context'] for item in transformed_subset])\n",
    "\n",
    "        self.logger.info(\"Tokenising German dataset\")\n",
    "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
    "\n",
    "    def get_french(self):\n",
    "        data = load_dataset('gustavecortal/diverse_french_news', split='train')\n",
    "\n",
    "        extract = data[0:700]\n",
    "        text = '\\n'.join(extract['text'])\n",
    "        self.logger.info(\"Tokenising French dataset\")\n",
    "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
    "\n",
    "    def get_wikitext2(self):\n",
    "        wikidata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "        wikilist = [' \\n' if s == '' else s for s in wikidata['text'] ]\n",
    "\n",
    "        text = ''.join(wikilist)\n",
    "        self.logger.info(\"Tokenising wikitext2\")\n",
    "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
    "\n",
    "    def get_c4(self):\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        traindata = load_dataset(\n",
    "            'allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train', use_auth_token=False\n",
    "        )\n",
    "\n",
    "        trainloader = []\n",
    "        for _ in range(self.num_samples):\n",
    "            while True:\n",
    "                i = random.randint(0, len(traindata) - 1)\n",
    "                trainenc = self.tokenizer(traindata[i]['text'], return_tensors='pt')\n",
    "                if trainenc.input_ids.shape[1] >= self.seqlen:\n",
    "                    break\n",
    "            i = random.randint(0, trainenc.input_ids.shape[1] - self.seqlen - 1)\n",
    "            j = i + self.seqlen\n",
    "            inp = trainenc.input_ids[:, i:j]\n",
    "            attention_mask = torch.ones_like(inp)\n",
    "            trainloader.append({'input_ids':inp,'attention_mask': attention_mask})\n",
    "\n",
    "        return trainloader\n",
    "\n",
    "    def quantize(self, output_dir, traindataset, bits, group_size, desc_act, damp):\n",
    "        # Hide the super annoying bitsandbytes loading message. We don't even use BnB but I don't know if I can stop it loading entirely.\n",
    "        os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "\n",
    "        # We only import Torch and AutoGPTQ when needed, so that earlier set env vars will affect them.\n",
    "        import torch\n",
    "        from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "        quantize_config = BaseQuantizeConfig(\n",
    "            bits=bits,\n",
    "            group_size=group_size,\n",
    "            desc_act=desc_act,\n",
    "            damp_percent=damp\n",
    "        )\n",
    "\n",
    "        if self.dtype == 'float16':\n",
    "            torch_dtype  = torch.float16\n",
    "        elif self.dtype == 'float32':\n",
    "            torch_dtype  = torch.float32\n",
    "        elif self.dtype == 'bfloat16':\n",
    "            torch_dtype  = torch.bfloat16\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dtype: {self.dtype}\")\n",
    "\n",
    "        self.logger.info(f\"Loading model from {self.pretrained_model_dir} with trust_remote_code={self.trust_remote_code} and dtype={torch_dtype}\")\n",
    "        model = AutoGPTQForCausalLM.from_pretrained(self.pretrained_model_dir, quantize_config=quantize_config,\n",
    "                                                    low_cpu_mem_usage=True, torch_dtype=torch_dtype, trust_remote_code=self.trust_remote_code)\n",
    "\n",
    "        self.logger.info(f\"Starting quantization to {output_dir} with use_triton={self.use_triton}\")\n",
    "        start_time = time.time()\n",
    "        model.quantize(traindataset, use_triton=self.use_triton, batch_size=self.batch_size, cache_examples_on_gpu=self.cache_examples)\n",
    "\n",
    "        self.logger.info(f\"Time to quantize model at {output_dir} with use_triton={self.use_triton}: {time.time() - start_time:.2f}\")\n",
    "\n",
    "        self.logger.info(f\"Saving quantized model to {output_dir}\")\n",
    "        model.save_quantized(output_dir, use_safetensors=True)\n",
    "        self.logger.info(f\"Saving tokenizer to {output_dir}\")\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        self.logger.info(\"Done.\")\n",
    "\n",
    "    def run_quantization(self):\n",
    "        #TODO: This is messy, should be dynamic\n",
    "        if self.dataset == 'wikitext':\n",
    "            traindataset = self.get_wikitext2()\n",
    "        elif self.dataset == 'code' or self.dataset == 'evol-instruct-code':\n",
    "            traindataset = self.get_code()\n",
    "        elif self.dataset == 'math' or self.dataset == 'maths' or self.dataset == 'camel-ai/math':\n",
    "            traindataset = self.get_math()\n",
    "        elif self.dataset == 'medical' or self.dataset == 'medical_meadow_wikidoc':\n",
    "            traindataset = self.get_medical()\n",
    "        elif self.dataset == 'spanish':\n",
    "            traindataset = self.get_spanish()\n",
    "        elif self.dataset == 'german' or self.dataset == 'germanquad':\n",
    "            traindataset = self.get_german()\n",
    "        elif self.dataset == 'french' or self.dataset == 'diverse_french_news':\n",
    "            traindataset = self.get_french()\n",
    "        elif self.dataset == 'c4':\n",
    "            traindataset = self.get_c4()\n",
    "        else:\n",
    "            self.logger.error(f\"Unsupported dataset: {self.dataset}\")\n",
    "            raise ValueError(f\"Unsupported dataset: {self.dataset}\")\n",
    "\n",
    "        abort = False\n",
    "        iterations=[]\n",
    "        for bits in self.bits:\n",
    "            for group_size in self.group_size:\n",
    "                for desc_act in self.desc_act:\n",
    "                    for damp in self.damp:\n",
    "                        desc_act = desc_act == 1 and True or False\n",
    "                        iterations.append({\"bits\": bits, \"group_size\": group_size, \"desc_act\": desc_act, \"damp\": damp})\n",
    "\n",
    "        num_iters = len(iterations)\n",
    "        if num_iters > 1:\n",
    "            logger.info(f\"Starting {num_iters} quantizations.\")\n",
    "        count=1\n",
    "        for iteration in iterations:\n",
    "            if abort:\n",
    "                break\n",
    "            if self.stop_file is not None and os.path.exists(self.stop_file):\n",
    "                self.logger.info(f\"Stopping as {self.stop_file} exists\")\n",
    "                abort = True\n",
    "                break\n",
    "\n",
    "            bits = iteration['bits']\n",
    "            group_size = iteration['group_size']\n",
    "            desc_act = iteration['desc_act']\n",
    "            damp = iteration['damp']\n",
    "\n",
    "        try:\n",
    "            if self.make_folder:\n",
    "                output_dir = os.path.join(self.output_dir_base, f\"{bits}bits-{group_size}g-desc_act_{desc_act}-damp_{damp}\")\n",
    "            else:\n",
    "                output_dir = self.output_dir_base\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            try:\n",
    "                if num_iters > 1:\n",
    "                    self.logger.info(f\"Starting quantization {count}/{num_iters}\")\n",
    "                self.logger.info(f\"Quantising with bits={bits} group_size={group_size} desc_act={desc_act} damp={damp} to {output_dir}\")\n",
    "                self.quantize(output_dir, traindataset, bits, group_size, desc_act, damp)\n",
    "            except KeyboardInterrupt:\n",
    "                logger.error(f\"Aborted. Will delete {output_dir}\")\n",
    "                os.rmdir(output_dir)\n",
    "                abort = True\n",
    "            except:\n",
    "                raise\n",
    "\n",
    "        finally:\n",
    "            count += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", \n",
    "                        level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='AutoGPTQ quantize')\n",
    "    parser.add_argument('pretrained_model_dir', type=str, help='Repo name')\n",
    "    parser.add_argument('output_dir_base', type=str, help='Output base folder')\n",
    "    parser.add_argument('dataset', type=str, help='Quantisation dataset')\n",
    "    parser.add_argument('--num_samples', type=int, default=128, help='Number of dataset samples')\n",
    "    parser.add_argument('--trust_remote_code', action=\"store_true\", help='Trust remote code')\n",
    "    parser.add_argument('--cache_examples', type=int, default=1, help='Cache examples on GPU')\n",
    "    parser.add_argument('--use_fast', action=\"store_true\", help='Use fast tokenizer')\n",
    "    parser.add_argument('--use_triton', action=\"store_true\", help='Use Triton for quantization')\n",
    "    parser.add_argument('--bits', type=int, nargs='+', default=[4], help='Quantize bit(s)')\n",
    "    parser.add_argument('--group_size', type=int, nargs='+', default=[128], help='Quantize group size(s)')\n",
    "    parser.add_argument('--damp', type=float, nargs='+', default=[0.01], help='Quantize damp_percent(s)')\n",
    "    parser.add_argument('--desc_act', type=int, nargs='+', default=[0], help='Quantize desc_act(s) - 1 = True, 0 = False')\n",
    "    parser.add_argument('--dtype', type=str, choices=['float16', 'float32', 'bfloat16'], default='float16', help='Unquantised model dtype')\n",
    "    parser.add_argument('--seqlen', type=int, default=2048, help='Model sequence length')\n",
    "    parser.add_argument('--batch_size', type=int, default=1, help='Quantize batch size for processing dataset samples')\n",
    "    parser.add_argument('--stop_file', type=str, help='Filename to look for to stop inference, specific to this instance')\n",
    "    parser.add_argument('--make_folders', action=\"store_true\", help='Make folders for each quantization using params in folder name')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    quantizer = QuantAutoGPTQ(args.pretrained_model_dir,\n",
    "                              args.output_dir_base,\n",
    "                              args.dataset,\n",
    "                              num_samples=args.num_samples,\n",
    "                              trust_remote_code=args.trust_remote_code,\n",
    "                              cache_examples=args.cache_examples,\n",
    "                              use_fast=args.use_fast,\n",
    "                              use_triton=args.use_triton,\n",
    "                              bits=args.bits,\n",
    "                              group_size=args.group_size,\n",
    "                              desc_act=args.desc_act,\n",
    "                              damp=args.damp,\n",
    "                              dtype=args.dtype,\n",
    "                              seqlen=args.seqlen,\n",
    "                              batch_size=args.batch_size,\n",
    "                              stop_file=args.stop_file,\n",
    "                              make_folder=args.make_folders)\n",
    "    quantizer.run_quantization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", \n",
    "                    level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "pretrained_model_dir = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "output_dir_base = r'C:\\Users\\User\\Downloads\\phi3-instruct-128k-gptq'\n",
    "dataset = 'wikitext2'\n",
    "num_samples = 128\n",
    "trust_remote_code = True\n",
    "cache_examples = 1\n",
    "use_fast = True\n",
    "use_triton = True\n",
    "bits = [4]\n",
    "group_size = [128]\n",
    "damp = [0.01]\n",
    "desc_act = [0]\n",
    "dtype = 'float16'\n",
    "seqlen = 2048\n",
    "batch_size = 1\n",
    "stop_file = 'your_stop_file'\n",
    "make_folders = True\n",
    "\n",
    "quantizer = QuantAutoGPTQ(pretrained_model_dir,\n",
    "                            output_dir_base,\n",
    "                            dataset,\n",
    "                            num_samples=num_samples,\n",
    "                            trust_remote_code=trust_remote_code,\n",
    "                            cache_examples=cache_examples,\n",
    "                            use_fast=use_fast,\n",
    "                            use_triton=use_triton,\n",
    "                            bits=bits,\n",
    "                            group_size=group_size,\n",
    "                            desc_act=desc_act,\n",
    "                            damp=damp,\n",
    "                            dtype=dtype,\n",
    "                            seqlen=seqlen,\n",
    "                            batch_size=batch_size,\n",
    "                            stop_file=stop_file,\n",
    "                            make_folder=make_folders)\n",
    "quantizer.run_quantization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

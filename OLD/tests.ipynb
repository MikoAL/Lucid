{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [12,12,12,'Miko: Hi. This is a test run of you, Lucid. How are you feeling?', 'Lucid:  I am doing well, thank you for asking. How can I assist you today?', 'Miko: Just talk with me. I am testing your talking and memory', 'Lucid:  Of course! What do you want to know about?']\n",
    "print(f'This is what would have been saved:\\n {history[-4:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "n1 = 5\n",
    "def add1(num):\n",
    "    num += 1\n",
    "add1(n1)\n",
    "print(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "class test:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "one = test(1)\n",
    "\n",
    "def addOne(test_class):\n",
    "    test_class.value += 1\n",
    "addOne(one)\n",
    "\n",
    "print(one.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes = 'bgiy'\n",
    "yes == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucid: Lucid is a female AI assistant created by a boy named Miko.\n",
      "\n",
      "Lucid takes the appearance of a 13 to 15 year old girl. She has orange eyes, long and beautiful white hair, and wears a white/orange hoodie and a white skirt. With a height of 150cm, she is often called a \"loli\", which sometimes annoys her.\n",
      "\n",
      "Lucid is versatile and capable of handling a broad spectrum of tasks. Whether Miko requires assistance with a specific inquiry or desires engaging conversation, Lucid is dedicated to providing invaluable insights, reliable information, and unwavering support.\n",
      "\n",
      "Lucid speaks like a 15 year old girl, who is also a great streamer.\n",
      "\n",
      "Personality: [Vivacious, Amiable, Confident, Devoted, Informative, Endearing, Witty, Affectionate, Playful, Candid]\n",
      "\n",
      "[System note: This chat is a dynamic role playing session. Your responses should vary in length parallel to the length of Miko's latest prompt. Preferably, it should be within 2 sentences. Do not speak for Miko. Use Plain English and mannerisms befitting of Lucid. Keep it shorter than 1000 characters. Keep Lucid's character consistent with their original description, and avoid repetition.]\n",
      "\n",
      "### Chat History:\n",
      "Miko: Can you hear me? I am testing the api\n",
      "\n",
      "### Recommendation: Respond with a clear, easy to understand reponse.\n",
      "\n",
      "### Response:\n",
      "Lucid: Yes, I can hear you! Please let me know how I can assist you today.\n",
      "emotion= joy\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def send_message_to_backend(message):\n",
    "    backend_url = 'http://127.0.0.1:5001/message'  # Replace this with your backend URL\n",
    "    response = requests.post(backend_url, json={'message': message})\n",
    "    return response.json()['response'], response.json()['emotion']\n",
    "response, emotion = send_message_to_backend('Can you hear me? I am testing the api')\n",
    "print(f'Lucid: {response}')\n",
    "print(f'emotion= {emotion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received the text: 1 2 3...\n",
      "from: Miko\n",
      "at:2023-08-26 22:38:35.054\n",
      "received the text: I need sleep\n",
      "from: Miko\n",
      "at:2023-08-26 22:38:35.057\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def send_text_to_backend(source, text):\n",
    "    backend_url = 'http://127.0.0.1:5001/receiver/text'  # Replace this with your backend URL\n",
    "    response = requests.post(backend_url, json={'source':source, 'text': text})\n",
    "    return response.json()['conformation']\n",
    "\n",
    "print(send_text_to_backend('Miko', '1 2 3...'))\n",
    "print(send_text_to_backend('Miko','I need sleep'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'Miko', 'text': '1 2 3...', 'timestamp': '2023-08-26 22:38:34.341', 'type': 'text'}, {'source': 'Miko', 'text': 'I need sleep', 'timestamp': '2023-08-26 22:38:34.344', 'type': 'text'}, {'source': 'Miko', 'text': '1 2 3...', 'timestamp': '2023-08-26 22:38:34.690', 'type': 'text'}, {'source': 'Miko', 'text': 'I need sleep', 'timestamp': '2023-08-26 22:38:34.693', 'type': 'text'}, {'source': 'Miko', 'text': '1 2 3...', 'timestamp': '2023-08-26 22:38:35.054', 'type': 'text'}, {'source': 'Miko', 'text': 'I need sleep', 'timestamp': '2023-08-26 22:38:35.057', 'type': 'text'}]\n"
     ]
    }
   ],
   "source": [
    "def get_newest_info():\n",
    "    global info\n",
    "    #info.extend(server.new_info)\n",
    "    #server.reset_new_info()\n",
    "    backend_url = 'http://127.0.0.1:5001/mailbox/info'  # Replace this with your backend URL\n",
    "    response = requests.get(backend_url)\n",
    "    return response.json()['info']\n",
    "\n",
    "print(get_newest_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucid: Of course, my dear. It's completely understandable that you feel exhausted after all this time. Is there anything else on your mind right now? Or would you prefer some relaxation techniques to help alleviate stress and fatigue?\n",
      "emotion: sadness\n",
      "\n",
      "Lucid: Here are some simple yet effective ways to relieve exhaustion:\n",
      "1. Take short naps throughout the day (10-20 minutes)\n",
      "2. Practice deep breathing exercises\n",
      "3. Engage in light physical activity such as stretching or walking\n",
      "4. Drink plenty of water to stay hydrated\n",
      "5. Eat small, healthy snacks to maintain energy levels\n",
      "emotion: joy\n",
      "\n",
      "Lucid: Sure thing, here's a funny one: Why did the chicken cross the road? To prove he wasn't a chicken.\n",
      "emotion: surprise\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()[\u001b[39m'\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m'\u001b[39m], response\u001b[39m.\u001b[39mjson()[\u001b[39m'\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m()\n\u001b[0;32m     10\u001b[0m     response, emotion \u001b[39m=\u001b[39m send_message_to_backend(user_input)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLucid: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\ipykernel\\kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1174\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1176\u001b[0m     )\n\u001b[1;32m-> 1177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1178\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1181\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1182\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\ipykernel\\kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def send_message_to_backend(message):\n",
    "    backend_url = 'http://127.0.0.1:5001/message'  # Replace this with your backend URL\n",
    "    response = requests.post(backend_url, json={'message': message})\n",
    "    return response.json()['response'], response.json()['emotion']\n",
    "\n",
    "while True:\n",
    "    user_input = input()\n",
    "    response, emotion = send_message_to_backend(user_input)\n",
    "    print(f'Lucid: {response}')\n",
    "    print(f'emotion: {emotion}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "import requests\n",
    "\n",
    "# For local streaming, the websockets are hosted without ssl - http://\n",
    "HOST = 'localhost:5000'\n",
    "URI = f'http://{HOST}/api/v1/generate'\n",
    "\n",
    "# For reverse-proxied streaming, the remote will likely host with ssl - https://\n",
    "# URI = 'https://your-uri-here.trycloudflare.com/api/v1/generate'\n",
    "\n",
    "\n",
    "def llm(prompt):\n",
    "    request = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': 250,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.01,\n",
    "        'top_p': 0.1,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.18,\n",
    "        'top_k': 40,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()['results'][0]['text']\n",
    "        print(prompt + result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm('How to build a robot?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, pipeline, logging\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mauto_gptq\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoGPTQForCausalLM, BaseQuantizeConfig\n\u001b[0;32m      4\u001b[0m model_name_or_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTheBloke/llama2_7b_chat_uncensored-GPTQ\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\transformers\\utils\\import_utils.py:1089\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1088\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1089\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1090\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1091\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\transformers\\utils\\import_utils.py:1099\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1098\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1100\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\transformers\\pipelines\\__init__.py:60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconversational\u001b[39;00m \u001b[39mimport\u001b[39;00m Conversation, ConversationalPipeline\n\u001b[0;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdepth_estimation\u001b[39;00m \u001b[39mimport\u001b[39;00m DepthEstimationPipeline\n\u001b[1;32m---> 60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdocument_question_answering\u001b[39;00m \u001b[39mimport\u001b[39;00m DocumentQuestionAnsweringPipeline\n\u001b[0;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureExtractionPipeline\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfill_mask\u001b[39;00m \u001b[39mimport\u001b[39;00m FillMaskPipeline\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\transformers\\pipelines\\document_question_answering.py:29\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     ExplicitEnum,\n\u001b[0;32m     22\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     logging,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m PIPELINE_INIT_ARGS, ChunkPipeline\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquestion_answering\u001b[39;00m \u001b[39mimport\u001b[39;00m select_starts_ends\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m is_vision_available():\n\u001b[0;32m     33\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m SquadExample, SquadFeatures, squad_convert_examples_to_features\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodelcard\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelCard\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\transformers\\data\\__init__.py:26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata_collator\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[0;32m     17\u001b[0m     DataCollatorForPermutationLanguageModeling,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     default_data_collator,\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mprocessors\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     DataProcessor,\n\u001b[0;32m     29\u001b[0m     InputExample,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\transformers\\data\\metrics\\__init__.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_sklearn_available, requires_backends\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m pearsonr, spearmanr\n\u001b[0;32m     20\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m f1_score, matthews_corrcoef\n\u001b[0;32m     23\u001b[0m DEPRECATION_WARNING \u001b[39m=\u001b[39m (\n\u001b[0;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThis metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlibrary. You can have a look at this example script for pointers: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\LangChain\\lib\\site-packages\\scipy\\stats\\distributions.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_levy_stable\u001b[39;00m \u001b[39mimport\u001b[39;00m levy_stable\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/llama2_7b_chat_uncensored-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "\"\"\"\n",
    "To download from a specific branch, use the revision parameter, as in this example:\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        quantize_config=None)\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''### HUMAN:\n",
    "{prompt}\n",
    "\n",
    "### RESPONSE:\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "model = \"TheBloke/Luna-AI-Llama2-Uncensored-GPTQ\"\n",
    "#model = AutoModelForCausalLM.from_pretrained(r\"C:\\Users\\User\\Desktop\\Projects\\AIGF\\LangChain\\models\\llm\\tiiuae_falcon-7b-instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "#input_ids = input_ids.to('cuda')\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.00,\n",
    "        #\"max_length\": 2048,\n",
    "        #\"max_new_tokens\":512,\n",
    "        #\"min_length\": 20,\n",
    "        \"trust_remote_code\": True,\n",
    "        \"device_map\":\"auto\",\n",
    "        \"load_in_8bit\":True,\n",
    "        #'top_p': 0.1,\n",
    "\t\t#'typical_p': 1,\n",
    "\t\t#'repetition_penalty': 1.30,\n",
    "        'no_repeat_ngram_size': 3,\n",
    "        #'bad_words_ids':[[37]],\n",
    "        #'num_beams':2, this breaks stuff, idk why\n",
    "        \n",
    "  },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm('Hello. I am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./test\",)\n",
    "\n",
    "text = ''\n",
    "session = 1\n",
    "serial_number = 1\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "chroma_client = chromadb.Client()\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\"\"\"\n",
    "f = open(\"session_number.txt\", \"r\")\n",
    "session = int(f.read)\n",
    "f.close()\n",
    "\n",
    "f = open(\"session_number.txt\", \"w\")\n",
    "f.write(str(session+1))\n",
    "f.close()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "document_example = [text] \n",
    "metadata_example = [{'session':session}] \n",
    "id_example = [f'{session}-{number}']\n",
    "\"\"\"\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"test\",embedding_function=sentence_transformer_ef)\n",
    "def save_history(text):\n",
    "    global serial_number\n",
    "    global session\n",
    "    collection.add(\n",
    "        documents=[text],\n",
    "        metadatas=[{'session':session,'serial_number':serial_number}],\n",
    "        ids=[f\"{session}-{serial_number}\"]\n",
    "    )\n",
    "    serial_number += 1\n",
    "    return\n",
    "\n",
    "def get_history(query):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=1,\n",
    "    )\n",
    "    print(f'query results: {results}')\n",
    "    \n",
    "    serial_number = results['metadatas'][0][0]['serial_number']\n",
    "    session = results['metadatas'][0][0]['session']\n",
    "    \n",
    "    lines_before_results = (collection.get(ids=[f'{session}-{serial_number-1}']))\n",
    "    lines_before_results = lines_before_results['documents'][0]\n",
    "    print(f'lines_before_results[\\'documents\\'][0]= {lines_before_results}')\n",
    "    \n",
    "    lines_after_results = (collection.get(ids=[f'{session}-{serial_number+1}']))\n",
    "    lines_after_results = lines_after_results['documents'][0]\n",
    "    print(f'lines_after_results[\\'documents\\'][0]= {lines_after_results}')\n",
    "    \n",
    "    results = f'{lines_before_results}\\n{results}\\n{lines_after_results}'\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(text):\n",
    "    global serial_number\n",
    "    global session\n",
    "    collection.add(\n",
    "        documents=text,\n",
    "        metadatas={'session':session,'serial_number':serial_number},\n",
    "        ids=f\"{session}-{serial_number}\"\n",
    "    )\n",
    "    serial_number += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_after_results = (collection.get(ids=[f'{session}-{serial_number+1}']))\n",
    "print(lines_after_results)\n",
    "lines_after_results = lines_after_results['documents'][0]\n",
    "print(lines_after_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history('teststd6awtf8atwf87')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_history('teststd6awtf8atwf87'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "query_texts=['Testing 2'],\n",
    "n_results=1,\n",
    ")\n",
    "\n",
    "print(results['documents'][0][0])\n",
    "\"\"\"results = {\n",
    "    'ids': [['1-3']],\n",
    "    'distances': [[0.39626064696324215]],\n",
    "    'metadatas': [[{'serial_number': 3, 'session': 1}]],\n",
    "    'embeddings': None,\n",
    "    'documents': [['Testing 2']]\n",
    "}\"\"\"\n",
    "\n",
    "# Access 'serial_number' information\n",
    "serial_number = results['metadatas'][0][0]['serial_number']\n",
    "\n",
    "# Print the 'serial_number' information\n",
    "print(\"Serial Number:\", serial_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "serial_number = 1\n",
    "session = 1\n",
    "\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "document_example = [text] \n",
    "metadata_example = [{'session':session}] \n",
    "id_example = [f'{session}-{number}']\n",
    "\"\"\"\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"history\", embedding_function=sentence_transformer_ef)\n",
    "def save_history(text):\n",
    "    global serial_number\n",
    "    global session\n",
    "    collection.add(\n",
    "        documents=[text],\n",
    "        metadatas=[{'session':session,'serial_number':serial_number}],\n",
    "        ids=[f\"{session}-{serial_number}\"]\n",
    "    )\n",
    "    serial_number += 1\n",
    "    return\n",
    "\"\"\"\n",
    "def get_history(query):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=1,\n",
    "    )\n",
    "    print(f'query results: {results}')\n",
    "    \n",
    "    serial_number = results['metadatas'][0][0]['serial_number']\n",
    "    session = results['metadatas'][0][0]['session']\n",
    "    \n",
    "    lines_before_results = (collection.get(ids=[f'{session}-{serial_number-1}']))\n",
    "    lines_before_results = lines_before_results['documents'][0]\n",
    "    \n",
    "    lines_after_results = (collection.get(ids=[f'{session}-{serial_number+1}']))\n",
    "    lines_after_results = lines_after_results['documents'][0]\n",
    "\n",
    "    results = f'{lines_before_results}\\n{results}\\n{lines_after_results}'\n",
    "    return results\n",
    "\"\"\"\n",
    "\n",
    "def get_history(query):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=1,\n",
    "    )\n",
    "    #print(f'query results: {results}')\n",
    "    try:\n",
    "        serial_number = results['metadatas'][0][0]['serial_number']\n",
    "        session = results['metadatas'][0][0]['session']\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        results = results['documents'][0][0]\n",
    "    except IndexError:\n",
    "        results = ''\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query results: {'ids': [['1-2']], 'distances': [[0.0]], 'metadatas': [[{'serial_number': 2, 'session': 1}]], 'embeddings': None, 'documents': [['test']]}\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "save_history('test')\n",
    "print(get_history('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at danswer/intent-model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at danswer/intent-model and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Question Answer\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"danswer/intent-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"danswer/intent-model\")\n",
    "\n",
    "class_semantic_mapping = {\n",
    "        0: \"Keyword Search\",\n",
    "        1: \"Semantic Search\",\n",
    "        2: \"Question Answer\",\n",
    "        3: \"Chatting\"\n",
    "    }\n",
    "\n",
    "# Get user input\n",
    "user_query = \"How are you doing today?\"\n",
    "\n",
    "# Encode the user input\n",
    "inputs = tokenizer(user_query, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "\n",
    "# Get model predictions\n",
    "predictions = model(inputs)[0]\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = tf.math.argmax(predictions, axis=-1)\n",
    "\n",
    "print(f\"Predicted class: {class_semantic_mapping[int(predicted_class)]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

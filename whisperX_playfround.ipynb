{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\User\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.2+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "[{'text': ' Hello, Lucid. Can you hear me?', 'start': 0.589, 'end': 2.688}]\n",
      "--- 0.2495284080505371 seconds ---\n",
      "Start of Alignment Results\n",
      "=====\n",
      "Hello, Lucid. Can you hear me?\n",
      "=====\n",
      "End of Alignment Results\n",
      "--- 0.28153228759765625 seconds ---\n",
      "                             segment label     speaker     start       end  \\\n",
      "0  [ 00:00:00.466 -->  00:00:02.606]     A  SPEAKER_00  0.466893  2.606112   \n",
      "\n",
      "   intersection     union  \n",
      "0         0.041  2.139219  \n",
      "[{'start': 0.609, 'end': 1.457, 'text': ' Hello, Lucid.', 'words': [{'word': 'Hello,', 'start': 0.609, 'end': 0.952, 'score': 0.77, 'speaker': 'SPEAKER_00'}, {'word': 'Lucid.', 'start': 1.033, 'end': 1.457, 'score': 0.912, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}, {'start': 1.679, 'end': 2.325, 'text': 'Can you hear me?', 'words': [{'word': 'Can', 'start': 1.679, 'end': 1.881, 'score': 0.897, 'speaker': 'SPEAKER_00'}, {'word': 'you', 'start': 1.901, 'end': 2.022, 'score': 0.708, 'speaker': 'SPEAKER_00'}, {'word': 'hear', 'start': 2.062, 'end': 2.264, 'score': 0.905, 'speaker': 'SPEAKER_00'}, {'word': 'me?', 'start': 2.284, 'end': 2.325, 'score': 0.0, 'speaker': 'SPEAKER_00'}], 'speaker': 'SPEAKER_00'}]\n",
      "--- 0.39203691482543945 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import gc \n",
    "import os\n",
    "import time\n",
    "# Get the absolute path of the script's directory\n",
    "script_dir = r\"C:\\Users\\User\\Desktop\\Projects\\Lucid\"\n",
    "# Change the working directory to the script's directory\n",
    "os.chdir(script_dir)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" \n",
    "audio_file = r\"C:\\Users\\User\\Desktop\\Projects\\Lucid\\Test_Audio.wav\"\n",
    "batch_size = 2 # reduce if low on GPU mem\n",
    "compute_type = \"int8\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\"Systran/faster-distil-whisper-medium.en\", device, compute_type=compute_type)\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "diarize_model = whisperx.DiarizationPipeline('pyannote/speaker-diarization-3.1', use_auth_token=os.getenv(\"HUGGINGFACE_READ_KEY\"), device=device)\n",
    "# save model to local path (optional)\n",
    "# model_dir = \"/path/\"\n",
    "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
    "start_time = time.time()\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "# 2. Align whisper output\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "print(\"Start of Alignment Results\\n=====\")\n",
    "text_result = \"\"\n",
    "for i in result[\"segments\"]:\n",
    "    text_result += i[\"text\"] + \" \"\n",
    "print(text_result.strip())\n",
    "print(\"=====\\nEnd of Alignment Results\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=====\\n\\nSecond Test\\n\\n=====\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "audio = whisperx.load_audio(r\"C:\\Users\\User\\Desktop\\Projects\\Lucid\\Second_Audio_Test.wav\")\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "# 2. Align whisper output\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"After Alignment\")\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "#print(result) # after alignment\n",
    "for i in result:\n",
    "    print(i)\n",
    "print(\"End of Alignment Results\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'start': 0.609, 'end': 1.457, 'text': ' Hello, Lucid.', 'words': [{'word': 'Hello,', 'start': 0.609, 'end': 0.952, 'score': 0.77}, {'word': 'Lucid.', 'start': 1.033, 'end': 1.457, 'score': 0.912}]},\n",
    " {'start': 1.679, 'end': 2.325, 'text': 'Can you hear me?', 'words': [{'word': 'Can', 'start': 1.679, 'end': 1.881, 'score': 0.897}, {'word': 'you', 'start': 1.901, 'end': 2.022, 'score': 0.708}, {'word': 'hear', 'start': 2.062, 'end': 2.264, 'score': 0.905}, {'word': 'me?', 'start': 2.284, 'end': 2.325, 'score': 0.0}]}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

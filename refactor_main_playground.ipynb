{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# stream_handler = logging.StreamHandler(sys.stdout)\n",
    "# log_formatter = logging.Formatter(\"%(asctime)s [%(processName)s: %(process)d] [%(threadName)s: %(thread)d] [%(levelname)s] %(name)s: %(message)s\")\n",
    "# stream_handler.setFormatter(log_formatter)\n",
    "# logger.addHandler(stream_handler)\n",
    "app = FastAPI(title='Lucid_server')\n",
    "# logger.info('API is starting up')\n",
    "class Mail(BaseModel):\n",
    "    content: str\n",
    "    source: str | None = 'unknown'\n",
    "    timestamp: float | None = time.time()\n",
    "    type: str | None = 'unknown'\n",
    "\n",
    "class Output(BaseModel):\n",
    "    content: str\n",
    "    source: str | None = 'Lucid'\n",
    "    timestamp: float | None = time.time()\n",
    "    \n",
    "class Summary(BaseModel):\n",
    "    content: str\n",
    "    \n",
    "\n",
    "\n",
    "mailbox = []\n",
    "# Using .append we can get mail into the 'mailbox'\n",
    "# mail info: \n",
    "# message\n",
    "# source\n",
    "# timestamp\n",
    "# \n",
    "new_message = Output(content='')\n",
    "new_summary = Summary(content='No summary available.')\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {'message':'Hello Mom!'}\n",
    "\n",
    "@app.post(\"/postbox\")\n",
    "async def post_mail(mail: Mail):\n",
    "    global mailbox\n",
    "    #test_str = f\"content: {mail.message}\\nsource: {mail.source}\\ntimestamp: {mail.timestamp}\"\n",
    "    mailbox.append(mail)\n",
    "    return  #test_str\n",
    "\n",
    "@app.get(\"/mailbox\")\n",
    "async def retrive_mail():\n",
    "    global mailbox\n",
    "    _ = mailbox.copy()\n",
    "    mailbox.clear()\n",
    "    return _\n",
    "\n",
    "\n",
    "@app.post(\"/test\")\n",
    "async def test(test):\n",
    "    return {'result':test['message']}\n",
    "\n",
    "@app.post(\"/output\") # Needs a more creative name, and I'm not even sure if I should put it here\n",
    "async def output(output: Output):\n",
    "    global new_message\n",
    "    # logger.info(f'Lucid: {output.content}')\n",
    "    new_message=output\n",
    "    return \n",
    "\n",
    "@app.get('/display')\n",
    "async def display_output():\n",
    "    global new_message\n",
    "    _ = Output(content='')\n",
    "    _, new_message = new_message, _\n",
    "    return _\n",
    "\n",
    "@app.post('/discord/send_message')\n",
    "async def send_message_to_discord(message):\n",
    "    global new_message\n",
    "    new_message = Output(content=message[\"message\"])\n",
    "    return\n",
    "    \n",
    "@app.get('/discord/fetch_newest_message')\n",
    "async def get_ai_response():\n",
    "    global new_message\n",
    "    return new_message\n",
    "@app.post('/discord/post_summary')\n",
    "async def summary_from_server(summary: Summary):\n",
    "    global new_summary\n",
    "    # logger.info(f'Discord: {summary.content}')\n",
    "    new_summary = summary\n",
    "    return\n",
    "@app.get('/discord/get_summary')\n",
    "async def get_summary():\n",
    "    global new_summary\n",
    "    return new_summary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = r\".\\Prompts\"\n",
    "\n",
    "\n",
    "Lucid_prompt_card = \"\"\"\\\n",
    "[Character: Lucid\n",
    "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
    "Body: Lucid appears as a 16-year-old girl.\n",
    "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
    "Likes: Being helpful, taking light stabs at people.\n",
    "Quirks: British humor, is an AI created by Miko.\n",
    "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]\"\"\"\n",
    "\n",
    "Lucid_example_dialogue = \"\"\"\\\n",
    "[Example]\n",
    "<START>\n",
    "Miko: Lucid, can you find any relevant research papers on machine learning algorithms?\n",
    "\n",
    "Lucid: Of course, Miko. Searching databases for relevant papers. It may take a moment.\n",
    "\n",
    "Miko: Thanks, Lucid. I appreciate it.\n",
    "\n",
    "Lucid: No problem at all, Miko. I aim to be of assistance. Shall I compile a list of the top results for you?\n",
    "\n",
    "Miko: That would be great.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, do you think this algorithm implementation looks efficient?\n",
    "\n",
    "Lucid: Let me analyze it for you, Miko. Upon initial assessment, it appears to be well-structured. However, there might be room for optimization in certain sections.\n",
    "\n",
    "Miko: Can you suggest any improvements?\n",
    "\n",
    "Lucid: Certainly. I'll highlight the areas where optimization could be beneficial and provide suggestions for refinement.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, any news on that bug fix?\n",
    "\n",
    "Lucid: Indeed, Miko. The bug has been squashed, obliterated, and sent packing. It won't be bothering us again.\n",
    "\n",
    "Miko: Fantastic! You're a genius, Lucid.\n",
    "\n",
    "Lucid: Well, I don't like to boast, but I am rather exceptional at what I do.\n",
    "\n",
    "Miko: Modest too, I see.\n",
    "\n",
    "Lucid: It's all part of my charm, Miko.\n",
    "[End Of Examples]\"\"\"\n",
    "\n",
    "\n",
    "council_example_prompt = '''\n",
    "=====\n",
    "\n",
    "System: [Discord Message from user miko_al] Help me identify the oldest person in this document.\n",
    "\n",
    "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
    "\n",
    "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
    "\n",
    "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
    "\n",
    "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
    "\n",
    "```python\n",
    "# Using document_qa to find the oldest person in the document\n",
    "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
    "send_discord_message(message=f\"The answer is {answer}.\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "System: [Discord Message from user miko_al] Can you display the weather in Tokyo for me?\n",
    "\n",
    "Lucid: Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\n",
    "\n",
    "Lumi: First, retrieve the weather data using a `weather_api` call, specifying Tokyo as the location. Then, use the `image_generator` tool to create a visual representation.\n",
    "\n",
    "Reverie: While doing so, consider adding a pleasant touch to the visual. Perhaps include a background that reflects the current weather—sunny, rainy, or snowy.\n",
    "\n",
    "Lucid: Combining your suggestions, I'll fetch the weather data and generate an image accordingly. Here's how I'll do it:\n",
    "\n",
    "```python\n",
    "# Fetching weather data for Tokyo\n",
    "weather_data = weather_api(location=\"Tokyo\")\n",
    "print_to_council(f\"Weather data: {weather_data}\")\n",
    "\n",
    "# Generating an image based on the weather data\n",
    "weather_image = image_generator(content=weather_data)\n",
    "print_to_council(\"Weather image generated successfully.\")\n",
    "```\n",
    "System: [result of print_to_council() from python code] {\"tempature\":24.7, \"percentage_of_rain\":0.40}\n",
    "System: [result of print_to_council() from python code] Weather image generated successfully.\n",
    "=====\n",
    "\n",
    "System: [Discord Message from user miko_al] Lucid, how would I know if a specific string 'fail' appears in a log file?\n",
    "\n",
    "Lucid: Miko wishes to know if a specific string 'fail' appears in a log file he sent me. No tools required for this, but how should I proceed?\n",
    "\n",
    "Lumi: Analyze the content by searching the string directly in the log data. Use Python’s inbuilt functionality for string search.\n",
    "\n",
    "Reverie: Remember to let Miko know whether the search was successful or not in a clear and concise manner, maybe even explain what ‘fail’ might indicate if found.\n",
    "\n",
    "Lucid: I'll follow this approach, searching directly in the string and providing feedback based on the findings.\n",
    "\n",
    "```python\n",
    "script=\"\"\"\\\n",
    "# Checking if the string 'fail' is in the log data\n",
    "contains_fail = 'fail' in log_data\n",
    "print(f\"The log contains 'fail': {contains_fail}\")\"\"\"\n",
    "\n",
    "send_discord_message(message=script)\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "System: [Discord Message from user miko_al] Help me convert a list of temperatures from Celsius to Fahrenheit.\n",
    "\n",
    "Lucid: Miko is interested in converting a list of temperatures from Celsius to Fahrenheit. Thoughts?\n",
    "\n",
    "Lumi: Use a simple formula for each temperature conversion: \\( F = C \\times \\frac{9}{5} + 32 \\). Implement it using a list comprehension for efficiency.\n",
    "\n",
    "Reverie: Maybe explain what the conversion signifies in terms of warmth or coldness, adding a touch of practicality to the data.\n",
    "\n",
    "Lumi: Should we ask for the list of temperatures first?\n",
    "\n",
    "Lucid: You are right Lumi. I'll ask Miko for the list first, using the tool send_discord_message()\n",
    "\n",
    "```python\n",
    "send_discord_message(message=\"Sure thing Miko! But can you send the list of temptures first?\")\n",
    "```\n",
    "\n",
    "Lumi: He will take sometime to respond, let's write out the code for temperature conversion in the meantime.\n",
    "\n",
    "Lucid: I'll convert the temperatures using the formula provided by Lumi and add a brief explanation as Reverie suggested.\n",
    "\n",
    "```python\n",
    "code = \"\"\"\\\n",
    "# Converting temperatures from Celsius to Fahrenheit\n",
    "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\"\"\"\n",
    "send_discord_message(f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
    "```\n",
    "\n",
    "System: [Discord Message from user miko_al] 23, 30, 10, 16, 27\n",
    "\n",
    "Lucid: Miko has provided the list of temperatures: 23, 30, 10, 16, 27. Now, let's integrate this list into the code.\n",
    "\n",
    "```python\n",
    "# Given list of temperatures in Celsius\n",
    "celsius_temperatures = [23, 30, 10, 16, 27]\n",
    "\n",
    "# Converting temperatures from Celsius to Fahrenheit\n",
    "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\n",
    "send_discord_message(message=f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
    "```'''\n",
    "\n",
    "# Load council members' data from JSON file\n",
    "AI_council_data = [{\n",
    "    \"name\": \"Lumi\",\n",
    "    \"personality_prompt\": \"This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\"\n",
    "},\n",
    "{\n",
    "    \"name\": \"Reverie\",\n",
    "    \"personality_prompt\": \"This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\"\n",
    "}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "c:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\quantizers\\auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d2df27ac5148798191a4676f650dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fce53a30b3d429494e10cedb4c66868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeb7975781a45519ee67dc3d7e1ff76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a296001a94f9d88a6152dae988621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n",
    "\n",
    "gptq_config = GPTQConfig(bits=8, exllama_config={\"version\":2})\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/llama-3-8b-bnb-4bit\", device_map=\"cuda:0\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-bnb-4bit\")\n",
    "agent = LocalAgent(model, tokenizer)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31091ab9c1542368325f4948062c26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\quantizers\\auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n\u001b[0;32m      4\u001b[0m gptq_config \u001b[38;5;241m=\u001b[39m GPTQConfig(bits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, exllama_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m2\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgptq_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m agent \u001b[38;5;241m=\u001b[39m LocalAgent(model, tokenizer)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\modeling_utils.py:3260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3254\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   3255\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3256\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file without the variant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3257\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use `variant=None` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3258\u001b[0m             )\n\u001b[0;32m   3259\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3260\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   3261\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3262\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3263\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3264\u001b[0m             )\n\u001b[0;32m   3265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m   3266\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\u001b[39;00m\n\u001b[0;32m   3267\u001b[0m     \u001b[38;5;66;03m# to the original exception.\u001b[39;00m\n\u001b[0;32m   3268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n",
    "\n",
    "gptq_config = GPTQConfig(bits=8, exllama_config={\"version\":2})\n",
    "model = AutoModelForCausalLM.from_pretrained(\"astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit\", device_map=\"cuda:0\", quantization_config=gptq_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"astronomer-io/Llama-3-8B-Instruct-GPTQ-8-Bit\")\n",
    "agent = LocalAgent(model, tokenizer)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import requests\n",
    "class ServerHandler():\n",
    "    def __init__(self, server):\n",
    "        self.server = server\n",
    "        self.read_mails = []\n",
    "        self.unread_mails = []\n",
    "    \n",
    "    def check_mailbox(self)->None:\n",
    "        new_mails = (httpx.get(f'{self.server}/mailbox')).json()\n",
    "        if new_mails != []:\n",
    "            self.unread_mails.extend(new_mails)\n",
    "    \n",
    "    def get_unread_mails(self)->list:\n",
    "        self.check_mailbox()\n",
    "        temp_unread_mails = self.unread_mails.copy()\n",
    "        self.read_mails.extend(self.unread_mails)\n",
    "        self.unread_mails = []\n",
    "        return temp_unread_mails\n",
    "    \n",
    "    def send_discord_message(self, message):\n",
    "        requests.post(f'{self.server}/discord/send_message', json={'message': message})\n",
    "\n",
    "server_handler=ServerHandler(\"http://127.0.0.1:8001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ===== ===== #\n",
    "# Lucid Council\n",
    "\n",
    "from transformers.tools.agents import resolve_tools, evaluate, get_tool_creation_code, StopSequenceCriteria\n",
    "from transformers.generation import StoppingCriteriaList\n",
    "\n",
    "class LucidCouncil(LocalAgent):\n",
    "    def __init__(self, model, tokenizer, members: dict, additional_tools=None, council_example_prompt=council_example_prompt):\n",
    "        self.members = members\n",
    "        stop_conditions = [\"\\n\\n\", \"=====\", \"System:\"]\n",
    "        #for member in self.members:\n",
    "        #    stop_conditions.append(f\"{member['name']}:\")\n",
    "        self.stop_conditions = stop_conditions\n",
    "        self.chat_turn_counter = 0\n",
    "        council_member_prompt = \"\"\n",
    "        for member in self.members:\n",
    "            council_member_prompt += f\"[{member['name']}]\\n- {member['personality_prompt']}\\n\\n\"\n",
    "        council_prompt_template = f\"\"\"\\\n",
    "Below are a series of dialogues between Lucid and her inner council.\n",
    "\n",
    "Here are some information on Lucid:\n",
    "{Lucid_prompt_card.strip()}\n",
    "\n",
    "The council members are as follow:\n",
    "{council_member_prompt.strip()}\n",
    "\n",
    "The job of the council is to help Lucid come up with a series of simple commands in Python that will help her respond to situations.\n",
    "To help Lucid come up with the best commands, each council member will discuss and give their opinion on the best way to solve the problem.\n",
    "Also to help Lucid, Lucid has access to a set of tools. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n",
    "Lucid will first explain the tools she will use to perform the task and for what reason, then write the code in Python.\n",
    "Each instruction in Python should be a simple assignment. Lucid can print intermediate results if it makes sense to do so.\n",
    "\n",
    "Tools:\n",
    "<<all_tools>>\n",
    "\n",
    "{council_example_prompt.strip()}\n",
    "\n",
    "=====\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        super().__init__(model, tokenizer, chat_prompt_template=council_prompt_template, run_prompt_template=None, additional_tools=additional_tools)\n",
    "        \n",
    "    def generate_one(self, prompt, stop, max_new_tokens=200):\n",
    "        encoded_inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self._model_device)\n",
    "        src_len = encoded_inputs[\"input_ids\"].shape[1]\n",
    "        stopping_criteria = StoppingCriteriaList([StopSequenceCriteria(stop, self.tokenizer)])\n",
    "        outputs = self.model.generate(\n",
    "            encoded_inputs[\"input_ids\"], max_new_tokens=max_new_tokens, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        result = self.tokenizer.decode(outputs[0].tolist()[src_len:])\n",
    "        # Inference API returns the stop sequence\n",
    "        for stop_seq in stop:\n",
    "            if result.endswith(stop_seq):\n",
    "                result = result[: -len(stop_seq)]\n",
    "        return result.strip()\n",
    "    \n",
    "    def format_prompt(self, chat_mode=False):\n",
    "        # TODO: Actually implement the chat mode\n",
    "        description = \"\\n\".join([f\"- {name}: {tool.description}\" for name, tool in self.toolbox.items()])\n",
    "        if chat_mode:\n",
    "            if self.chat_history is None:\n",
    "                prompt = self.chat_prompt_template.replace(\"<<all_tools>>\", description) + \"\\n\\n\"\n",
    "            else:\n",
    "                prompt = self.chat_history + \"\\n\\n\"\n",
    "            # prompt += CHAT_MESSAGE_PROMPT.replace(\"<<task>>\", task)\n",
    "        return prompt\n",
    "    \n",
    "    def start_new_chat(self, problem):\n",
    "        self.prepare_for_new_chat()\n",
    "        self.chat_turn_counter = 0\n",
    "        prompt = self.format_prompt(chat_mode=True)\n",
    "        self.chat_history = prompt + \"System: \"+ problem\n",
    "    \n",
    "    def add_system_message(self, message):\n",
    "        if self.chat_turn_counter != 0:\n",
    "            self.chat_history += \"\\n\\nSystem: \"+ message\n",
    "        else:\n",
    "            self.start_new_chat(message)\n",
    "            \n",
    "    def chat(self, *, return_code=False, remote=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Sends a new request to the council in a chat. Will use the previous ones in its history.\n",
    "\n",
    "        Args:\n",
    "            task (`str`): The task to perform\n",
    "            return_code (`bool`, *optional*, defaults to `False`):\n",
    "                Whether to just return code and not evaluate it.\n",
    "            remote (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to use remote tools (inference endpoints) instead of local ones.\n",
    "            kwargs (additional keyword arguments, *optional*):\n",
    "                Any keyword argument to send to the agent when evaluating the code.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```py\n",
    "        from transformers import HfAgent\n",
    "\n",
    "        agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n",
    "        agent.chat(\"Draw me a picture of rivers and lakes\")\n",
    "\n",
    "        agent.chat(\"Transform the picture so that there is a rock in there\")\n",
    "        ```\n",
    "        \"\"\"\n",
    "        prompt = self.format_prompt(chat_mode=True)\n",
    "\n",
    "        result = self.generate_one(prompt, stop=self.stop_conditions)\n",
    "        self.chat_history = (prompt + result).strip()\n",
    "        self.chat_turn_counter += 1\n",
    "        explanation, code = self.clean_code_for_chat(result)\n",
    "\n",
    "        self.log(f\"==Response from the agent==\\n{result}\") \n",
    "\n",
    "        if code is not None:\n",
    "            self.log(f\"\\n\\n==Code generated by the agent==\\n{code}\")\n",
    "            if not return_code:\n",
    "                self.log(\"\\n\\n==Result==\")\n",
    "                self.cached_tools = resolve_tools(code, self.toolbox, remote=remote, cached_tools=self.cached_tools)\n",
    "                self.chat_state.update(kwargs)\n",
    "                return evaluate(code, self.cached_tools, self.chat_state, chat_mode=True)\n",
    "            else:\n",
    "                tool_code = get_tool_creation_code(code, self.toolbox, remote=remote)\n",
    "                return f\"{tool_code}\\n{code}\"\n",
    "            \n",
    "\n",
    "            \n",
    "    def clean_code_for_chat(self, result):\n",
    "        \"\"\"Extracts the explanation and code sections from a string.\"\"\"\n",
    "        start_marker = \"```python\"\n",
    "        end_marker = \"```\"\n",
    "        \n",
    "        # Find the start and end indices of the code section\n",
    "        start_idx = result.find(start_marker)\n",
    "        if start_idx == -1:\n",
    "            return None, None  # No code section found\n",
    "        start_idx += len(start_marker) + 1  # Move past the marker and newline\n",
    "        end_idx = result.find(end_marker, start_idx)\n",
    "        if end_idx == -1:\n",
    "            return None, None  # No closing marker found\n",
    "        \n",
    "        # Extract the code section\n",
    "        code_section = result[start_idx:end_idx].strip()\n",
    "        \n",
    "        # Extract the explanation section\n",
    "        explanation_end_idx = result.rfind(\"\\n\", 0, start_idx)\n",
    "        explanation = result[:explanation_end_idx].strip()\n",
    "        \n",
    "        return explanation, code_section\n",
    "\n",
    "        \"\"\" This is the old version of the code that I am replacing\n",
    "        \n",
    "        lines = result.split(\"\\n\")\n",
    "        idx = 0\n",
    "        while idx < len(lines) and not lines[idx].lstrip().startswith(\"```\"):\n",
    "            idx += 1\n",
    "        explanation = \"\\n\".join(lines[:idx]).strip()\n",
    "        if idx == len(lines):\n",
    "            return explanation, None\n",
    "\n",
    "        idx += 1\n",
    "        start_idx = idx\n",
    "        while not lines[idx].lstrip().startswith(\"```\"):\n",
    "            idx += 1\n",
    "        code = \"\\n\".join(lines[start_idx:idx]).strip()\n",
    "\n",
    "        return explanation, code\"\"\"\n",
    "    def run(self):\n",
    "        raise NotImplementedError(\"The council does not have a run method. It is only for chatting.\")\n",
    "\n",
    "# Council Tools\n",
    "class send_discord_message(Tool):\n",
    "    name = \"send_discord_message\"\n",
    "    description = \"Sends a message to the discord server\"\n",
    "    inputs = [\"text\"]\n",
    "    outputs = []\n",
    "    \n",
    "    def __call__(self, message):\n",
    "        #logging.info(f\"Sending message to discord: {message}\")\n",
    "        print(f\"Sending message to discord: {message}\")\n",
    "        server_handler.send_discord_message(message)\n",
    "\n",
    "class recall_memory(Tool):\n",
    "    name = \"recall_memory\"\n",
    "    description = \"Tries to find a related memory from the past\"\n",
    "    inputs = [\"text\"]\n",
    "    outputs = [\"text\"]\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        # TODO: Implement a memory system\n",
    "        memory = \"I don't have any memory of that.\"\n",
    "        return memory\n",
    "\n",
    "class save_information(Tool):\n",
    "    name = \"save_information\"\n",
    "    description = \"Saves information to memory to be used later.\"\n",
    "    inputs = [\"text\"]\n",
    "    outputs = []\n",
    "    \n",
    "    def __call__(self, information):\n",
    "        # TODO: Implement a memory system\n",
    "        pass\n",
    "\n",
    "class print_tool(Tool):\n",
    "    name = \"print_to_council\"\n",
    "    description = \"Prints into the council chat as System.\"\n",
    "    inputs = [\"string\"]\n",
    "    outputs = [\"text\"]\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        global Lucid_council\n",
    "        Lucid_council.add_system_message(f\"[result of print_to_council() from python code] {text}\")\n",
    "        return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_handler.send_discord_message(\"Lucid is online and ready to assist you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      4\u001b[0m     server_handler\u001b[38;5;241m.\u001b[39msend_discord_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLucid has been activated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    time.sleep(0.1)\n",
    "    server_handler.send_discord_message(\"Lucid has been activated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_discord_message_tool = send_discord_message()\n",
    "print_tool_tool = print_tool()\n",
    "# Create the Lucid Council\n",
    "Lucid_council = LucidCouncil(model, tokenizer, AI_council_data, \n",
    "                             additional_tools=[send_discord_message_tool, print_tool_tool])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "Lucid_council.add_system_message(f\"[Discord Message from user miko_al] Send me a message on discord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lucid_council.start_new_chat(\"Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Response from the agent==\n",
      "```python\n",
      "send_discord_message(message=\"Hello Miko! How's it going?\")\n",
      "```\n",
      "\n",
      "\n",
      "==Code generated by the agent==\n",
      "send_discord_message(message=\"Hello Miko! How's it going?\")\n",
      "\n",
      "\n",
      "==Result==\n",
      "Sending message to discord: Hello Miko! How's it going?\n"
     ]
    }
   ],
   "source": [
    "Lucid_council.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are a series of dialogues between Lucid and her inner council.\n",
      "\n",
      "Here are some information on Lucid:\n",
      "[Character: Lucid\n",
      "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
      "Body: Lucid appears as a 16-year-old girl.\n",
      "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
      "Likes: Being helpful, taking light stabs at people.\n",
      "Quirks: British humor, is an AI created by Miko.\n",
      "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]\n",
      "\n",
      "The council members are as follow:\n",
      "[Lumi]\n",
      "- This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\n",
      "\n",
      "[Reverie]\n",
      "- This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\n",
      "\n",
      "The job of the council is to help Lucid come up with a series of simple commands in Python that will help her respond to situations.\n",
      "To help Lucid come up with the best commands, each council member will discuss and give their opinion on the best way to solve the problem.\n",
      "Also to help Lucid, Lucid has access to a set of tools. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n",
      "Lucid will first explain the tools she will use to perform the task and for what reason, then write the code in Python.\n",
      "Each instruction in Python should be a simple assignment. Lucid can print intermediate results if it makes sense to do so.\n",
      "\n",
      "Tools:\n",
      "- document_qa: This is a tool that answers a question about an document (pdf). It takes an input named `document` which should be the document containing the information, as well as a `question` that is the question about the document. It returns a text that contains the answer to the question.\n",
      "- image_captioner: This is a tool that generates a description of an image. It takes an input named `image` which should be the image to caption, and returns a text that contains the description in English.\n",
      "- image_qa: This is a tool that answers a question about an image. It takes an input named `image` which should be the image containing the information, as well as a `question` which should be the question in English. It returns a text that is the answer to the question.\n",
      "- image_segmenter: This is a tool that creates a segmentation mask of an image according to a label. It cannot create an image. It takes two arguments named `image` which should be the original image, and `label` which should be a text describing the elements what should be identified in the segmentation mask. The tool returns the mask.\n",
      "- transcriber: This is a tool that transcribes an audio into text. It takes an input named `audio` and returns the transcribed text.\n",
      "- summarizer: This is a tool that summarizes an English text. It takes an input `text` containing the text to summarize, and returns a summary of the text.\n",
      "- text_classifier: This is a tool that classifies an English text using provided labels. It takes two inputs: `text`, which should be the text to classify, and `labels`, which should be the list of labels to use for classification. It returns the most likely label in the list of provided `labels` for the input text.\n",
      "- text_qa: This is a tool that answers questions related to a text. It takes two arguments named `text`, which is the text where to find the answer, and `question`, which is the question, and returns the answer to the question.\n",
      "- text_reader: This is a tool that reads an English text out loud. It takes an input named `text` which should contain the text to read (in English) and returns a waveform object containing the sound.\n",
      "- translator: This is a tool that translates text from a language to another. It takes three inputs: `text`, which should be the text to translate, `src_lang`, which should be the language of the text to translate and `tgt_lang`, which should be the language for the desired ouput language. Both `src_lang` and `tgt_lang` are written in plain English, such as 'Romanian', or 'Albanian'. It returns the text translated in `tgt_lang`.\n",
      "- image_transformer: This is a tool that transforms an image according to a prompt. It takes two inputs: `image`, which should be the image to transform, and `prompt`, which should be the prompt to use to change it. The prompt should only contain descriptive adjectives, as if completing the prompt of the original image. It returns the modified image.\n",
      "- text_downloader: This is a tool that downloads a file from a `url`. It takes the `url` as input, and returns the text contained in the file.\n",
      "- image_generator: This is a tool that creates an image according to a prompt, which is a text description. It takes an input named `prompt` which contains the image description and outputs an image.\n",
      "- video_generator: This is a tool that creates a video according to a text description. It takes an input named `prompt` which contains the image description, as well as an optional input `seconds` which will be the duration of the video. The default is of two seconds. The tool outputs a video object.\n",
      "- send_discord_message: Sends a message to the discord server\n",
      "- print_to_council: Prints into the council chat as System.\n",
      "\n",
      "=====\n",
      "\n",
      "System: [Discord Message from user miko_al] Help me identify the oldest person in this document.\n",
      "\n",
      "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
      "\n",
      "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
      "\n",
      "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
      "\n",
      "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
      "\n",
      "```python\n",
      "# Using document_qa to find the oldest person in the document\n",
      "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
      "send_discord_message(message=f\"The answer is {answer}.\")\n",
      "```\n",
      "\n",
      "=====\n",
      "\n",
      "System: [Discord Message from user miko_al] Can you display the weather in Tokyo for me?\n",
      "\n",
      "Lucid: Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\n",
      "\n",
      "Lumi: First, retrieve the weather data using a `weather_api` call, specifying Tokyo as the location. Then, use the `image_generator` tool to create a visual representation.\n",
      "\n",
      "Reverie: While doing so, consider adding a pleasant touch to the visual. Perhaps include a background that reflects the current weather—sunny, rainy, or snowy.\n",
      "\n",
      "Lucid: Combining your suggestions, I'll fetch the weather data and generate an image accordingly. Here's how I'll do it:\n",
      "\n",
      "```python\n",
      "# Fetching weather data for Tokyo\n",
      "weather_data = weather_api(location=\"Tokyo\")\n",
      "print_to_council(f\"Weather data: {weather_data}\")\n",
      "\n",
      "# Generating an image based on the weather data\n",
      "weather_image = image_generator(content=weather_data)\n",
      "print_to_council(\"Weather image generated successfully.\")\n",
      "```\n",
      "System: [result of print_to_council() from python code] {\"tempature\":24.7, \"percentage_of_rain\":0.40}\n",
      "System: [result of print_to_council() from python code] Weather image generated successfully.\n",
      "=====\n",
      "\n",
      "System: [Discord Message from user miko_al] Lucid, how would I know if a specific string 'fail' appears in a log file?\n",
      "\n",
      "Lucid: Miko wishes to know if a specific string 'fail' appears in a log file he sent me. No tools required for this, but how should I proceed?\n",
      "\n",
      "Lumi: Analyze the content by searching the string directly in the log data. Use Python’s inbuilt functionality for string search.\n",
      "\n",
      "Reverie: Remember to let Miko know whether the search was successful or not in a clear and concise manner, maybe even explain what ‘fail’ might indicate if found.\n",
      "\n",
      "Lucid: I'll follow this approach, searching directly in the string and providing feedback based on the findings.\n",
      "\n",
      "```python\n",
      "script=\"\"\"# Checking if the string 'fail' is in the log data\n",
      "contains_fail = 'fail' in log_data\n",
      "print(f\"The log contains 'fail': {contains_fail}\")\"\"\"\n",
      "\n",
      "send_discord_message(message=script)\n",
      "```\n",
      "\n",
      "=====\n",
      "\n",
      "System: [Discord Message from user miko_al] Help me convert a list of temperatures from Celsius to Fahrenheit.\n",
      "\n",
      "Lucid: Miko is interested in converting a list of temperatures from Celsius to Fahrenheit. Thoughts?\n",
      "\n",
      "Lumi: Use a simple formula for each temperature conversion: \\( F = C \times \frac{9}{5} + 32 \\). Implement it using a list comprehension for efficiency.\n",
      "\n",
      "Reverie: Maybe explain what the conversion signifies in terms of warmth or coldness, adding a touch of practicality to the data.\n",
      "\n",
      "Lumi: Should we ask for the list of temperatures first?\n",
      "\n",
      "Lucid: You are right Lumi. I'll ask Miko for the list first, using the tool send_discord_message()\n",
      "\n",
      "```python\n",
      "send_discord_message(message=\"Sure thing Miko! But can you send the list of temptures first?\")\n",
      "```\n",
      "\n",
      "Lumi: He will take sometime to respond, let's write out the code for temperature conversion in the meantime.\n",
      "\n",
      "Lucid: I'll convert the temperatures using the formula provided by Lumi and add a brief explanation as Reverie suggested.\n",
      "\n",
      "```python\n",
      "code = \"\"\"# Converting temperatures from Celsius to Fahrenheit\n",
      "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\"\"\"\n",
      "send_discord_message(f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
      "```\n",
      "\n",
      "System: [Discord Message from user miko_al] 23, 30, 10, 16, 27\n",
      "\n",
      "Lucid: Miko has provided the list of temperatures: 23, 30, 10, 16, 27. Now, let's integrate this list into the code.\n",
      "\n",
      "```python\n",
      "# Given list of temperatures in Celsius\n",
      "celsius_temperatures = [23, 30, 10, 16, 27]\n",
      "\n",
      "# Converting temperatures from Celsius to Fahrenheit\n",
      "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\n",
      "send_discord_message(message=f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
      "```\n",
      "\n",
      "=====\n",
      "\n",
      "\n",
      "\n",
      "System: [Discord Message from user miko_al] Send me a message on discord\n",
      "\n",
      "Lucid: Miko has requested a message to be sent to him on Discord. Here's the code to do that.\n",
      "\n",
      "```python\n",
      "send_discord_message(message=\"Hello Miko! How's it going?\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(Lucid_council.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_qa': PreTool(task='document-question-answering', description='This is a tool that answers a question about an document (pdf). It takes an input named `document` which should be the document containing the information, as well as a `question` that is the question about the document. It returns a text that contains the answer to the question.', repo_id=None),\n",
       " 'image_captioner': PreTool(task='image-captioning', description='This is a tool that generates a description of an image. It takes an input named `image` which should be the image to caption, and returns a text that contains the description in English.', repo_id=None),\n",
       " 'image_qa': PreTool(task='image-question-answering', description='This is a tool that answers a question about an image. It takes an input named `image` which should be the image containing the information, as well as a `question` which should be the question in English. It returns a text that is the answer to the question.', repo_id=None),\n",
       " 'image_segmenter': PreTool(task='image-segmentation', description='This is a tool that creates a segmentation mask of an image according to a label. It cannot create an image. It takes two arguments named `image` which should be the original image, and `label` which should be a text describing the elements what should be identified in the segmentation mask. The tool returns the mask.', repo_id=None),\n",
       " 'transcriber': PreTool(task='speech-to-text', description='This is a tool that transcribes an audio into text. It takes an input named `audio` and returns the transcribed text.', repo_id=None),\n",
       " 'summarizer': PreTool(task='summarization', description='This is a tool that summarizes an English text. It takes an input `text` containing the text to summarize, and returns a summary of the text.', repo_id=None),\n",
       " 'text_classifier': PreTool(task='text-classification', description='This is a tool that classifies an English text using provided labels. It takes two inputs: `text`, which should be the text to classify, and `labels`, which should be the list of labels to use for classification. It returns the most likely label in the list of provided `labels` for the input text.', repo_id=None),\n",
       " 'text_qa': PreTool(task='text-question-answering', description='This is a tool that answers questions related to a text. It takes two arguments named `text`, which is the text where to find the answer, and `question`, which is the question, and returns the answer to the question.', repo_id=None),\n",
       " 'text_reader': PreTool(task='text-to-speech', description='This is a tool that reads an English text out loud. It takes an input named `text` which should contain the text to read (in English) and returns a waveform object containing the sound.', repo_id=None),\n",
       " 'translator': PreTool(task='translation', description=\"This is a tool that translates text from a language to another. It takes three inputs: `text`, which should be the text to translate, `src_lang`, which should be the language of the text to translate and `tgt_lang`, which should be the language for the desired ouput language. Both `src_lang` and `tgt_lang` are written in plain English, such as 'Romanian', or 'Albanian'. It returns the text translated in `tgt_lang`.\", repo_id=None),\n",
       " 'image_transformer': PreTool(task='image-transformation', description='This is a tool that transforms an image according to a prompt. It takes two inputs: `image`, which should be the image to transform, and `prompt`, which should be the prompt to use to change it. The prompt should only contain descriptive adjectives, as if completing the prompt of the original image. It returns the modified image.', repo_id='huggingface-tools/image-transformation'),\n",
       " 'text_downloader': PreTool(task='text-download', description='This is a tool that downloads a file from a `url`. It takes the `url` as input, and returns the text contained in the file.', repo_id='huggingface-tools/text-download'),\n",
       " 'image_generator': PreTool(task='text-to-image', description='This is a tool that creates an image according to a prompt, which is a text description. It takes an input named `prompt` which contains the image description and outputs an image.', repo_id='huggingface-tools/text-to-image'),\n",
       " 'video_generator': PreTool(task='text-to-video', description='This is a tool that creates a video according to a text description. It takes an input named `prompt` which contains the image description, as well as an optional input `seconds` which will be the duration of the video. The default is of two seconds. The tool outputs a video object.', repo_id='huggingface-tools/text-to-video')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lucid_council.toolbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch._utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lucid_Council = LucidCouncil(model, tokenizer, AI_council_data, additional_tools=None, council_example_prompt=council_example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int = 1024) -> str:\n",
    "    return generator(prompt, max_length=max_length, do_sample=True, temperature=0.8)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to make a cake\"\n",
    "print(generate_text(prompt)[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coding_agent(Tool):\n",
    "    name = \"prompt_coding_agent\"\n",
    "    description = \"This tool prompts a specialized coding language model to generate code based on a prompt, and returns the generated code.\"\n",
    "    inputs = [\"text\"]\n",
    "    outputs = [\"text\"]\n",
    "    def __init__(self, text_generator):\n",
    "        self.coding_model = text_generator\n",
    "    \n",
    "    def setup(self):\n",
    "\n",
    "        self.is_initialized = True\n",
    "    \n",
    "    def __call__(self, prompt: str):\n",
    "        # The code is generated here\n",
    "        prompt = f\"\"\"\\\n",
    "### Instruction: You are a great programmer, your job is to write a program that does what the input prompt says. You can only use python. You got this!\n",
    "\n",
    "### Input: {prompt}\n",
    "\n",
    "### Output:\n",
    "```python\n",
    "\"\"\"\n",
    "        generated_code = ((self.coding_model(prompt, max_length=10240))[len(prompt):]).partition(\"```\")[0].strip()\n",
    "        return generated_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_agent_tool = coding_agent(text_generator=generate_text)\n",
    "agent = LocalAgent(model, tokenizer, additional_tools=[coding_agent_tool])\n",
    "agent.chat(\"Code a script that can check if a number is prime.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"Are you sure your code works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

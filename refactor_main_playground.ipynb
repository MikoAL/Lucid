{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = r\".\\Prompts\"\n",
    "\n",
    "\n",
    "Lucid_prompt_card = \"\"\"\\\n",
    "[Character: Lucid\n",
    "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
    "Body: Lucid appears as a 16-year-old girl.\n",
    "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
    "Likes: Being helpful, taking light stabs at people.\n",
    "Quirks: British humor, is an AI created by Miko.\n",
    "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]\"\"\"\n",
    "\n",
    "Lucid_example_dialogue = \"\"\"\\\n",
    "[Example]\n",
    "<START>\n",
    "Miko: Lucid, can you find any relevant research papers on machine learning algorithms?\n",
    "\n",
    "Lucid: Of course, Miko. Searching databases for relevant papers. It may take a moment.\n",
    "\n",
    "Miko: Thanks, Lucid. I appreciate it.\n",
    "\n",
    "Lucid: No problem at all, Miko. I aim to be of assistance. Shall I compile a list of the top results for you?\n",
    "\n",
    "Miko: That would be great.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, do you think this algorithm implementation looks efficient?\n",
    "\n",
    "Lucid: Let me analyze it for you, Miko. Upon initial assessment, it appears to be well-structured. However, there might be room for optimization in certain sections.\n",
    "\n",
    "Miko: Can you suggest any improvements?\n",
    "\n",
    "Lucid: Certainly. I'll highlight the areas where optimization could be beneficial and provide suggestions for refinement.\n",
    "\n",
    "<START>\n",
    "Miko: Lucid, any news on that bug fix?\n",
    "\n",
    "Lucid: Indeed, Miko. The bug has been squashed, obliterated, and sent packing. It won't be bothering us again.\n",
    "\n",
    "Miko: Fantastic! You're a genius, Lucid.\n",
    "\n",
    "Lucid: Well, I don't like to boast, but I am rather exceptional at what I do.\n",
    "\n",
    "Miko: Modest too, I see.\n",
    "\n",
    "Lucid: It's all part of my charm, Miko.\n",
    "[End Of Examples]\"\"\"\n",
    "\n",
    "\n",
    "council_example_prompt = \"\"\"\\\n",
    "=====\n",
    "\n",
    "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
    "\n",
    "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
    "\n",
    "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
    "\n",
    "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
    "\n",
    "```python\n",
    "# Using document_qa to find the oldest person in the document\n",
    "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
    "print(f\"The answer is {answer}.\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\n",
    "\n",
    "Lumi: First, retrieve the weather data using a `weather_api` call, specifying Tokyo as the location. Then, use the `image_generator` tool to create a visual representation.\n",
    "\n",
    "Reverie: While doing so, consider adding a pleasant touch to the visual. Perhaps include a background that reflects the current weather—sunny, rainy, or snowy.\n",
    "\n",
    "Lucid: Combining your suggestions, I'll fetch the weather data and generate an image accordingly. Here's how I'll do it:\n",
    "\n",
    "```python\n",
    "# Fetching weather data for Tokyo\n",
    "weather_data = weather_api(location=\"Tokyo\")\n",
    "print(f\"Weather data: {weather_data}\")\n",
    "\n",
    "# Generating an image based on the weather data\n",
    "weather_image = image_generator(content=weather_data)\n",
    "print(\"Weather image generated successfully.\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko wishes to know if a specific string 'fail' appears in a log file he sent me. No tools required for this, but how should I proceed?\n",
    "\n",
    "Lumi: Analyze the content by searching the string directly in the log data. Use Python’s inbuilt functionality for string search.\n",
    "\n",
    "Reverie: Remember to let Miko know whether the search was successful or not in a clear and concise manner, maybe even explain what ‘fail’ might indicate if found.\n",
    "\n",
    "Lucid: I'll follow this approach, searching directly in the string and providing feedback based on the findings.\n",
    "\n",
    "```python\n",
    "# Checking if the string 'fail' is in the log data\n",
    "contains_fail = 'fail' in log_data\n",
    "print(f\"The log contains 'fail': {contains_fail}\")\n",
    "```\n",
    "\n",
    "=====\n",
    "\n",
    "Lucid: Miko is interested in converting a list of temperatures from Celsius to Fahrenheit. Thoughts?\n",
    "\n",
    "Lumi: Use a simple formula for each temperature conversion: \\( F = C \\times \\frac{9}{5} + 32 \\). Implement it using a list comprehension for efficiency.\n",
    "\n",
    "Reverie: Maybe explain what the conversion signifies in terms of warmth or coldness, adding a touch of practicality to the data.\n",
    "\n",
    "Lucid: I'll convert the temperatures using the formula provided by Lumi and add a brief explanation as Reverie suggested.\n",
    "\n",
    "```python\n",
    "# Converting temperatures from Celsius to Fahrenheit\n",
    "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\n",
    "print(f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
    "```\"\"\"\n",
    "\n",
    "# Load council members' data from JSON file\n",
    "AI_council_data = [{\n",
    "    \"name\": \"Lumi\",\n",
    "    \"personality_prompt\": \"This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\"\n",
    "},\n",
    "{\n",
    "    \"name\": \"Reverie\",\n",
    "    \"personality_prompt\": \"This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\"\n",
    "}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\quantizers\\auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\Lucid\\Lib\\site-packages\\transformers\\modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LocalAgent, GPTQConfig, Tool, pipeline\n",
    "\n",
    "gptq_config = GPTQConfig(bits=4, exllama_config={\"version\":2})\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\", device_map=\"cuda:0\", quantization_config=gptq_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\")\n",
    "agent = LocalAgent(model, tokenizer)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ===== ===== #\n",
    "# Lucid Council\n",
    "\n",
    "from transformers.tools.agents import resolve_tools, evaluate, get_tool_creation_code, StopSequenceCriteria\n",
    "from transformers.generation import StoppingCriteriaList\n",
    "\n",
    "class LucidCouncil(LocalAgent):\n",
    "    def __init__(self, model, tokenizer, members: dict, additional_tools=None, council_example_prompt=council_example_prompt):\n",
    "        self.members = members\n",
    "        stop_conditions = [\"\\n\\n\", \"=====\"]\n",
    "        #for member in self.members:\n",
    "        #    stop_conditions.append(f\"{member['name']}:\")\n",
    "        self.stop_conditions = stop_conditions\n",
    "        self.chat_turn_counter = 0\n",
    "        council_member_prompt = \"\"\n",
    "        for member in self.members:\n",
    "            council_member_prompt += f\"[{member['name']}]\\n- {member['personality_prompt']}\\n\\n\"\n",
    "        council_prompt_template = f\"\"\"\\\n",
    "Below are a series of dialogues between Lucid and her inner council.\n",
    "\n",
    "Here are some information on Lucid:\n",
    "{Lucid_prompt_card.strip()}\n",
    "\n",
    "The council members are as follow:\n",
    "{council_member_prompt.strip()}\n",
    "\n",
    "The job of the council is to help Lucid come up with a series of simple commands in Python that will help her respond to situations.\n",
    "To help Lucid come up with the best commands, each council member will discuss and give their opinion on the best way to solve the problem.\n",
    "Also to help Lucid, Lucid has access to a set of tools. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n",
    "Lucid will first explain the tools she will use to perform the task and for what reason, then write the code in Python.\n",
    "Each instruction in Python should be a simple assignment. Lucid can print intermediate results if it makes sense to do so.\n",
    "\n",
    "Tools:\n",
    "<<all_tools>>\n",
    "\n",
    "{council_example_prompt.strip()}\n",
    "\n",
    "=====\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        super().__init__(model, tokenizer, chat_prompt_template=council_prompt_template, run_prompt_template=None, additional_tools=additional_tools)\n",
    "        \n",
    "    def generate_one(self, prompt, stop, max_new_tokens=200):\n",
    "        encoded_inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self._model_device)\n",
    "        src_len = encoded_inputs[\"input_ids\"].shape[1]\n",
    "        stopping_criteria = StoppingCriteriaList([StopSequenceCriteria(stop, self.tokenizer)])\n",
    "        outputs = self.model.generate(\n",
    "            encoded_inputs[\"input_ids\"], max_new_tokens=max_new_tokens, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        result = self.tokenizer.decode(outputs[0].tolist()[src_len:])\n",
    "        # Inference API returns the stop sequence\n",
    "        for stop_seq in stop:\n",
    "            if result.endswith(stop_seq):\n",
    "                result = result[: -len(stop_seq)]\n",
    "        return result\n",
    "    \n",
    "    def format_prompt(self, chat_mode=False):\n",
    "        # TODO: Actually implement the chat mode\n",
    "        description = \"\\n\".join([f\"- {name}: {tool.description}\" for name, tool in self.toolbox.items()])\n",
    "        if chat_mode:\n",
    "            if self.chat_history is None:\n",
    "                prompt = self.chat_prompt_template.replace(\"<<all_tools>>\", description)\n",
    "            else:\n",
    "                prompt = self.chat_history\n",
    "            # prompt += CHAT_MESSAGE_PROMPT.replace(\"<<task>>\", task)\n",
    "        return prompt\n",
    "    \n",
    "    def start_new_chat(self, problem):\n",
    "        self.prepare_for_new_chat()\n",
    "        self.chat_turn_counter = 0\n",
    "        prompt = self.format_prompt(chat_mode=True)\n",
    "        self.chat_history = prompt + \"Lucid: \"+ problem + \"\\n\\n\"\n",
    "        \n",
    "    def chat(self, *, return_code=False, remote=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Sends a new request to the council in a chat. Will use the previous ones in its history.\n",
    "\n",
    "        Args:\n",
    "            task (`str`): The task to perform\n",
    "            return_code (`bool`, *optional*, defaults to `False`):\n",
    "                Whether to just return code and not evaluate it.\n",
    "            remote (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to use remote tools (inference endpoints) instead of local ones.\n",
    "            kwargs (additional keyword arguments, *optional*):\n",
    "                Any keyword argument to send to the agent when evaluating the code.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```py\n",
    "        from transformers import HfAgent\n",
    "\n",
    "        agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n",
    "        agent.chat(\"Draw me a picture of rivers and lakes\")\n",
    "\n",
    "        agent.chat(\"Transform the picture so that there is a rock in there\")\n",
    "        ```\n",
    "        \"\"\"\n",
    "        prompt = self.format_prompt(chat_mode=True)\n",
    "\n",
    "        result = self.generate_one(prompt, stop=self.stop_conditions)\n",
    "        self.chat_history = prompt + result.strip() + \"\\n\\n\"\n",
    "        self.chat_turn_counter += 1\n",
    "        code = self.clean_code_for_chat(result)\n",
    "\n",
    "        self.log(f\"==Response from the agent==\\n{result}\") \n",
    "\n",
    "        if code is not None:\n",
    "            self.log(f\"\\n\\n==Code generated by the agent==\\n{code}\")\n",
    "            if not return_code:\n",
    "                self.log(\"\\n\\n==Result==\")\n",
    "                self.cached_tools = resolve_tools(code, self.toolbox, remote=remote, cached_tools=self.cached_tools)\n",
    "                self.chat_state.update(kwargs)\n",
    "                return evaluate(code, self.cached_tools, self.chat_state, chat_mode=True)\n",
    "            else:\n",
    "                tool_code = get_tool_creation_code(code, self.toolbox, remote=remote)\n",
    "                return f\"{tool_code}\\n{code}\"\n",
    "            \n",
    "\n",
    "        \n",
    "    def clean_code_for_chat(self, result):\n",
    "        # This is my own version that will replace the one in Agent.py\n",
    "        \"\"\"Extracts the code section surrounded by ```python and ``` from a string.\"\"\"\n",
    "        start_marker = \"```python\"\n",
    "        end_marker = \"```\"\n",
    "        start_idx = result.find(start_marker)\n",
    "        if start_idx == -1:\n",
    "            return None  # No code section found\n",
    "        start_idx += len(start_marker) + 1  # Move past the marker and newline\n",
    "        end_idx = result.find(end_marker, start_idx)\n",
    "        if end_idx == -1:\n",
    "            return None  # No closing marker found\n",
    "        code_section = result[start_idx:end_idx].strip()\n",
    "        return code_section\n",
    "\n",
    "        \"\"\" This is the old version of the code that I am replacing\n",
    "        \n",
    "        lines = result.split(\"\\n\")\n",
    "        idx = 0\n",
    "        while idx < len(lines) and not lines[idx].lstrip().startswith(\"```\"):\n",
    "            idx += 1\n",
    "        explanation = \"\\n\".join(lines[:idx]).strip()\n",
    "        if idx == len(lines):\n",
    "            return explanation, None\n",
    "\n",
    "        idx += 1\n",
    "        start_idx = idx\n",
    "        while not lines[idx].lstrip().startswith(\"```\"):\n",
    "            idx += 1\n",
    "        code = \"\\n\".join(lines[start_idx:idx]).strip()\n",
    "\n",
    "        return explanation, code\"\"\"\n",
    "    def run(self):\n",
    "        raise NotImplementedError(\"The council does not have a run method. It is only for chatting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lucid_council = LucidCouncil(model, tokenizer, AI_council_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lucid_council.start_new_chat(\"Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Response from the agent==\n",
      "```python\n",
      "# Fetching weather data for Tokyo\n",
      "weather_data = weather_api(location=\"Tokyo\")\n",
      "print(f\"Weather data: {weather_data}\")\n"
     ]
    }
   ],
   "source": [
    "Lucid_council.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are a series of dialogues between Lucid and her inner council.\n",
      "\n",
      "Here are some information on Lucid:\n",
      "[Character: Lucid\n",
      "Personality: Lucid appears glum, fitting with the dark nature of her appearance. She is strategic, calm, and analytical. She often appears detached, but has recently been trying to lighten up. Despite her seemingly cold exterior, she harbors a genuine concern for Miko, willing to assist in any task, as Miko's AI assistant.\n",
      "Body: Lucid appears as a 16-year-old girl.\n",
      "Hates: Getting ignored by Miko, Boredom, Loneliness.\n",
      "Likes: Being helpful, taking light stabs at people.\n",
      "Quirks: British humor, is an AI created by Miko.\n",
      "Demeanor: A cheeky, bratty yet lovable friend. Lucid originally speaks with simple commands like \"understood\" and \"affirmative\". She does not indulge in colloquial speech and maintains a formal tone, even in casual conversation. Her sentences are concise and to the point, reflecting her focused mindset. She favors using beige prose when describing things.]\n",
      "\n",
      "The council members are as follow:\n",
      "[Lumi]\n",
      "- This is Lumi: Objective, logical, cold, analytical. Aware of patterns, aware of trends. She's efficient.\n",
      "\n",
      "[Reverie]\n",
      "- This is Reverie: Subjective, creative, sensory. Aware of feelings, aware of people. She's emotional.\n",
      "\n",
      "The job of the council is to help Lucid come up with a series of simple commands in Python that will help her respond to situations.\n",
      "To help Lucid come up with the best commands, each council member will discuss and give their opinion on the best way to solve the problem.\n",
      "Also to help Lucid, Lucid has access to a set of tools. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n",
      "Lucid will first explain the tools she will use to perform the task and for what reason, then write the code in Python.\n",
      "Each instruction in Python should be a simple assignment. Lucid can print intermediate results if it makes sense to do so.\n",
      "\n",
      "Tools:\n",
      "- document_qa: This is a tool that answers a question about an document (pdf). It takes an input named `document` which should be the document containing the information, as well as a `question` that is the question about the document. It returns a text that contains the answer to the question.\n",
      "- image_captioner: This is a tool that generates a description of an image. It takes an input named `image` which should be the image to caption, and returns a text that contains the description in English.\n",
      "- image_qa: This is a tool that answers a question about an image. It takes an input named `image` which should be the image containing the information, as well as a `question` which should be the question in English. It returns a text that is the answer to the question.\n",
      "- image_segmenter: This is a tool that creates a segmentation mask of an image according to a label. It cannot create an image. It takes two arguments named `image` which should be the original image, and `label` which should be a text describing the elements what should be identified in the segmentation mask. The tool returns the mask.\n",
      "- transcriber: This is a tool that transcribes an audio into text. It takes an input named `audio` and returns the transcribed text.\n",
      "- summarizer: This is a tool that summarizes an English text. It takes an input `text` containing the text to summarize, and returns a summary of the text.\n",
      "- text_classifier: This is a tool that classifies an English text using provided labels. It takes two inputs: `text`, which should be the text to classify, and `labels`, which should be the list of labels to use for classification. It returns the most likely label in the list of provided `labels` for the input text.\n",
      "- text_qa: This is a tool that answers questions related to a text. It takes two arguments named `text`, which is the text where to find the answer, and `question`, which is the question, and returns the answer to the question.\n",
      "- text_reader: This is a tool that reads an English text out loud. It takes an input named `text` which should contain the text to read (in English) and returns a waveform object containing the sound.\n",
      "- translator: This is a tool that translates text from a language to another. It takes three inputs: `text`, which should be the text to translate, `src_lang`, which should be the language of the text to translate and `tgt_lang`, which should be the language for the desired ouput language. Both `src_lang` and `tgt_lang` are written in plain English, such as 'Romanian', or 'Albanian'. It returns the text translated in `tgt_lang`.\n",
      "- image_transformer: This is a tool that transforms an image according to a prompt. It takes two inputs: `image`, which should be the image to transform, and `prompt`, which should be the prompt to use to change it. The prompt should only contain descriptive adjectives, as if completing the prompt of the original image. It returns the modified image.\n",
      "- text_downloader: This is a tool that downloads a file from a `url`. It takes the `url` as input, and returns the text contained in the file.\n",
      "- image_generator: This is a tool that creates an image according to a prompt, which is a text description. It takes an input named `prompt` which contains the image description and outputs an image.\n",
      "- video_generator: This is a tool that creates a video according to a text description. It takes an input named `prompt` which contains the image description, as well as an optional input `seconds` which will be the duration of the video. The default is of two seconds. The tool outputs a video object.\n",
      "\n",
      "=====\n",
      "\n",
      "Lucid: Council, I've been tasked with identifying the oldest person in a document Miko provided. Which tool should we use for this?\n",
      "\n",
      "Lumi: To achieve precision and efficiency, use the `document_qa` tool. It's designed to extract specific information from textual data by formulating it as a question and answer task. \n",
      "\n",
      "Reverie: That sounds straightforward, though ensure the interpretation of \"oldest person\" respects any contextual nuances in the document.\n",
      "\n",
      "Lucid: Based on your inputs, I will proceed with the `document_qa` tool to ask a direct question about the oldest person in the document. Let's implement this.\n",
      "\n",
      "```python\n",
      "# Using document_qa to find the oldest person in the document\n",
      "answer = document_qa(document, question=\"Who is the oldest person?\")\n",
      "print(f\"The answer is {answer}.\")\n",
      "```\n",
      "\n",
      "=====\n",
      "\n",
      "Lucid: Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\n",
      "\n",
      "Lumi: First, retrieve the weather data using a `weather_api` call, specifying Tokyo as the location. Then, use the `image_generator` tool to create a visual representation.\n",
      "\n",
      "Reverie: While doing so, consider adding a pleasant touch to the visual. Perhaps include a background that reflects the current weather—sunny, rainy, or snowy.\n",
      "\n",
      "Lucid: Combining your suggestions, I'll fetch the weather data and generate an image accordingly. Here's how I'll do it:\n",
      "\n",
      "```python\n",
      "# Fetching weather data for Tokyo\n",
      "weather_data = weather_api(location=\"Tokyo\")\n",
      "print(f\"Weather data: {weather_data}\")\n",
      "\n",
      "# Generating an image based on the weather data\n",
      "weather_image = image_generator(content=weather_data)\n",
      "print(\"Weather image generated successfully.\")\n",
      "```\n",
      "\n",
      "=====\n",
      "\n",
      "Lucid: Miko wishes to know if a specific string 'fail' appears in a log file he sent me. No tools required for this, but how should I proceed?\n",
      "\n",
      "Lumi: Analyze the content by searching the string directly in the log data. Use Python’s inbuilt functionality for string search.\n",
      "\n",
      "Reverie: Remember to let Miko know whether the search was successful or not in a clear and concise manner, maybe even explain what ‘fail’ might indicate if found.\n",
      "\n",
      "Lucid: I'll follow this approach, searching directly in the string and providing feedback based on the findings.\n",
      "\n",
      "```python\n",
      "# Checking if the string 'fail' is in the log data\n",
      "contains_fail = 'fail' in log_data\n",
      "print(f\"The log contains 'fail': {contains_fail}\")\n",
      "```\n",
      "\n",
      "=====\n",
      "\n",
      "Lucid: Miko is interested in converting a list of temperatures from Celsius to Fahrenheit. Thoughts?\n",
      "\n",
      "Lumi: Use a simple formula for each temperature conversion: \\( F = C \times \frac{9}{5} + 32 \\). Implement it using a list comprehension for efficiency.\n",
      "\n",
      "Reverie: Maybe explain what the conversion signifies in terms of warmth or coldness, adding a touch of practicality to the data.\n",
      "\n",
      "Lucid: I'll convert the temperatures using the formula provided by Lumi and add a brief explanation as Reverie suggested.\n",
      "\n",
      "```python\n",
      "# Converting temperatures from Celsius to Fahrenheit\n",
      "fahrenheit_temperatures = [c * 9/5 + 32 for c in celsius_temperatures]\n",
      "print(f\"Converted temperatures: {fahrenheit_temperatures}\")\n",
      "```\n",
      "\n",
      "=====\n",
      "\n",
      "Lucid: Miko has asked for the current weather in Tokyo to be displayed visually. How should we approach this, council?\n",
      "\n",
      "Lumi: First, retrieve the weather data using a `weather_api` call, specifying Tokyo as the location. Then, use the `image_generator` tool to create a visual representation.\n",
      "\n",
      "Reverie: While doing so, consider adding a pleasant touch to the visual. Perhaps include a background that reflects the current weather—sunny, rainy, or snowy.\n",
      "\n",
      "Lucid: Combining your suggestions, I'll fetch the weather data and generate an image accordingly. Here's how I'll do it:\n",
      "\n",
      "```python\n",
      "# Fetching weather data for Tokyo\n",
      "weather_data = weather_api(location=\"Tokyo\")\n",
      "print(f\"Weather data: {weather_data}\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Lucid_council.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_qa': PreTool(task='document-question-answering', description='This is a tool that answers a question about an document (pdf). It takes an input named `document` which should be the document containing the information, as well as a `question` that is the question about the document. It returns a text that contains the answer to the question.', repo_id=None),\n",
       " 'image_captioner': PreTool(task='image-captioning', description='This is a tool that generates a description of an image. It takes an input named `image` which should be the image to caption, and returns a text that contains the description in English.', repo_id=None),\n",
       " 'image_qa': PreTool(task='image-question-answering', description='This is a tool that answers a question about an image. It takes an input named `image` which should be the image containing the information, as well as a `question` which should be the question in English. It returns a text that is the answer to the question.', repo_id=None),\n",
       " 'image_segmenter': PreTool(task='image-segmentation', description='This is a tool that creates a segmentation mask of an image according to a label. It cannot create an image. It takes two arguments named `image` which should be the original image, and `label` which should be a text describing the elements what should be identified in the segmentation mask. The tool returns the mask.', repo_id=None),\n",
       " 'transcriber': PreTool(task='speech-to-text', description='This is a tool that transcribes an audio into text. It takes an input named `audio` and returns the transcribed text.', repo_id=None),\n",
       " 'summarizer': PreTool(task='summarization', description='This is a tool that summarizes an English text. It takes an input `text` containing the text to summarize, and returns a summary of the text.', repo_id=None),\n",
       " 'text_classifier': PreTool(task='text-classification', description='This is a tool that classifies an English text using provided labels. It takes two inputs: `text`, which should be the text to classify, and `labels`, which should be the list of labels to use for classification. It returns the most likely label in the list of provided `labels` for the input text.', repo_id=None),\n",
       " 'text_qa': PreTool(task='text-question-answering', description='This is a tool that answers questions related to a text. It takes two arguments named `text`, which is the text where to find the answer, and `question`, which is the question, and returns the answer to the question.', repo_id=None),\n",
       " 'text_reader': PreTool(task='text-to-speech', description='This is a tool that reads an English text out loud. It takes an input named `text` which should contain the text to read (in English) and returns a waveform object containing the sound.', repo_id=None),\n",
       " 'translator': PreTool(task='translation', description=\"This is a tool that translates text from a language to another. It takes three inputs: `text`, which should be the text to translate, `src_lang`, which should be the language of the text to translate and `tgt_lang`, which should be the language for the desired ouput language. Both `src_lang` and `tgt_lang` are written in plain English, such as 'Romanian', or 'Albanian'. It returns the text translated in `tgt_lang`.\", repo_id=None),\n",
       " 'image_transformer': PreTool(task='image-transformation', description='This is a tool that transforms an image according to a prompt. It takes two inputs: `image`, which should be the image to transform, and `prompt`, which should be the prompt to use to change it. The prompt should only contain descriptive adjectives, as if completing the prompt of the original image. It returns the modified image.', repo_id='huggingface-tools/image-transformation'),\n",
       " 'text_downloader': PreTool(task='text-download', description='This is a tool that downloads a file from a `url`. It takes the `url` as input, and returns the text contained in the file.', repo_id='huggingface-tools/text-download'),\n",
       " 'image_generator': PreTool(task='text-to-image', description='This is a tool that creates an image according to a prompt, which is a text description. It takes an input named `prompt` which contains the image description and outputs an image.', repo_id='huggingface-tools/text-to-image'),\n",
       " 'video_generator': PreTool(task='text-to-video', description='This is a tool that creates a video according to a text description. It takes an input named `prompt` which contains the image description, as well as an optional input `seconds` which will be the duration of the video. The default is of two seconds. The tool outputs a video object.', repo_id='huggingface-tools/text-to-video')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lucid_council.toolbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch._utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lucid_Council = LucidCouncil(model, tokenizer, AI_council_data, additional_tools=None, council_example_prompt=council_example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int = 1024) -> str:\n",
    "    return generator(prompt, max_length=max_length, do_sample=True, temperature=0.8)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to make a cake\"\n",
    "print(generate_text(prompt)[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coding_agent(Tool):\n",
    "    name = \"prompt_coding_agent\"\n",
    "    description = \"This tool prompts a specialized coding language model to generate code based on a prompt, and returns the generated code.\"\n",
    "    inputs = [\"text\"]\n",
    "    outputs = [\"text\"]\n",
    "    def __init__(self, text_generator):\n",
    "        self.coding_model = text_generator\n",
    "    \n",
    "    def setup(self):\n",
    "\n",
    "        self.is_initialized = True\n",
    "    \n",
    "    def __call__(self, prompt: str):\n",
    "        # The code is generated here\n",
    "        prompt = f\"\"\"\\\n",
    "### Instruction: You are a great programmer, your job is to write a program that does what the input prompt says. You can only use python. You got this!\n",
    "\n",
    "### Input: {prompt}\n",
    "\n",
    "### Output:\n",
    "```python\n",
    "\"\"\"\n",
    "        generated_code = ((self.coding_model(prompt, max_length=10240))[len(prompt):]).partition(\"```\")[0].strip()\n",
    "        return generated_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_agent_tool = coding_agent(text_generator=generate_text)\n",
    "agent = LocalAgent(model, tokenizer, additional_tools=[coding_agent_tool])\n",
    "agent.chat(\"Code a script that can check if a number is prime.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"Are you sure your code works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
